{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework_4_iannwtf.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22Aa8t7uiVCp"
      },
      "source": [
        "### **Data Set**\n",
        "\n",
        "In this homework we would like you to be sophisticated: The dataset we want you to\n",
        "work on is the wine quality dataset, containing a set of collected features for each wine\n",
        "and a rank of that wine as found by ’wine experts’ on the scale of 1 to 10.\n",
        "You can also find this dataset in the Tensorflow Datasets Collection , however, as tfds\n",
        "has been proven to be a bit difficult at times, this week we would like you to create your\n",
        "own Tensoflow Dataset from a .csv using pandas 1\n",
        ". Do not worry, this is very easy!\n",
        "\n",
        "**1.1 Load Data into a Dataframe**\n",
        "\n",
        "You can download the .csv and store it on your local computer or directly read in the\n",
        "url when working with pandas. Read the data into a pandas dataframe. 2\n",
        "1\n",
        "Wine Quality Dataset: https://archive.ics.uci.edu/ml/\n",
        "machine-learning-databases/wine-quality/winequality-red.csv\n",
        "Make yourself familiar with the dataset.\n",
        "• What keys are there?\n",
        "• What should be the input and what should be the target for our NN? 3\n",
        "\n",
        "**1.2 Create a Tensorflow Dataset and a Dataset Pipeline**\n",
        "\n",
        "Split the dataset into a train, test and validation split. 4\n",
        "Separate the labels from the input and store them accordingly. 5\n",
        "Out of the resulting dataframes, build a Tensorflow Dataset.\n",
        "6\n",
        "Now we want to make this a Binary Classification Task: As we are not as sophisticated as our wine experts, we only care about good wine vs. bad wine. Write a function\n",
        "make binary(target) that receives a target and returns a target fit for a binary classification task. 7 You can hardcode (i.e. fix) a threshold or use statistics obtained from\n",
        "your data, e.g. the median ranking of all wines.8\n",
        "Create a Data Pipeline with all the necessary steps. You should at least map the\n",
        "make binary() function to the dataset and apply batching.\n",
        "\n",
        "\n",
        "1. Don’t forget to import pandas! Usually, ’import pandas as pd is used, thus in the future we will\n",
        "refer to pandas with pd\n",
        "2. Check out pd.readcsv()and think about how to set the delimiter argument 14\n",
        "3. The column ’quality’ contains the respective targets to the feature vector specified by all previous\n",
        "columns i.e. each point of the dataset should have 11 values (fixed acidity, volatile acidity,...) and one\n",
        "target (quality).\n",
        "4. check out df.sample(frac=, randomstate =) and df.drop()\n",
        "5. check out pandas df.drop( ,axis=)\n",
        "6. again check out tf.data.Dataset.fromtensorslices()\n",
        "7. this function should compare the rank to some threshold and return a 0 for bad and a 1 for good wine\n",
        "8. if you use the mean, you may notice that your dataset becomes extremely imbalanced\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4fgiQkiiZiQ"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "url=\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "\n",
        "df=pd.read_csv(filepath_or_buffer = url, sep  =\";\")\n",
        "df.head(10)\n",
        "# Index(['fixed acidity', 'volatile acidity', 'citric acid', \n",
        "#                             'residual sugar', 'chlorides', 'free sulfur dioxide',\n",
        "#                             'total sulfur dioxide', 'density', 'pH', 'sulphates', \n",
        "#                             'alcohol', 'quality'],  dtype='object')\n",
        "\n",
        "features = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
        "            'chlorides', 'free sulfur dioxide','total sulfur dioxide', 'density',\n",
        "            'pH', 'sulphates','alcohol']\n",
        "label = \"quality\"\n",
        "\n",
        "# train 60%, test 20%, validation 20%\n",
        "\n",
        "train =  df.sample(frac=0.6, random_state=0)\n",
        "test = df.sample(frac=0.2, random_state=1)\n",
        "validation = df.sample(frac=0.2, random_state=2)\n",
        "\n",
        "# print(train.head(10))\n",
        "# print(test.head(10))\n",
        "# print(validation.head(10))\n",
        "\n",
        "# creating train data and label\n",
        "train_data = train.drop([label],axis = 1)\n",
        "train_label = train[[label]]\n",
        "# train_data.head(10)\n",
        "# train_label.head(10)\n",
        "\n",
        "\n",
        "# creating test data and label\n",
        "test_data = test.drop([label],axis = 1)\n",
        "test_label = test[[label]]\n",
        "\n",
        "\n",
        "# creating validation data and label\n",
        "validation_data = validation.drop([label],axis = 1)\n",
        "validation_label = validation[[label]]\n",
        "\n",
        "# possible problem to encounter, label has to be removed from every dataframe and feed into the NN\n",
        "\n",
        "# remark, not sure if the split is corect as I think, they have some overlaps which is not expected for the NN\n",
        "#print(train.info(),test.info(),validation.info(), df.info())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwaNBQfXj8zt"
      },
      "source": [
        "\n",
        "### 2 Model\n",
        "You can pretty much recycle the model you defined for last week’s task with some small\n",
        "modifications.\n",
        "• As the task is maybe a bit simpler and you are aiming to explore the effect of\n",
        "optimization techniques, also try to make your model a bit smaller. The baseline\n",
        "model should barely be able to perform sufficiently good at the task (better than\n",
        "random).\n",
        "• As we now are dealing with a binary classification task, you have to change the\n",
        "configuration of the output layer. 9\n",
        "\n",
        "9. you only need 1 output neuron and you should use something like a sigmoid as an activation function. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84a2okRUkiVA"
      },
      "source": [
        "import tensorflow as tf\n",
        "class MyModel(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(32, activation=tf.nn.relu)\n",
        "        self.dense2 = tf.keras.layers.Dense(32, activation=tf.nn.relu)\n",
        "        self.output_layer = tf.keras.layers.Dense(1, activation= sigmoid())\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# Custom Layer\n",
        "class SimpleDense(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, units=8):\n",
        "        super(SimpleDense, self).__init__()\n",
        "        self.units = units\n",
        "        self.activation = tf.nn.softmax\n",
        "\n",
        "    def build(self, input_shape): \n",
        "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                               initializer='random_normal',\n",
        "                               trainable=True)\n",
        "        self.b = self.add_weight(shape=(self.units,),\n",
        "                               initializer='random_normal',\n",
        "                               trainable=True)\n",
        "\n",
        "    def call(self, inputs): \n",
        "        x = tf.matmul(inputs, self.w) + self.b\n",
        "        x = self.activation(x)\n",
        "        return x"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3OkvehTkiuO"
      },
      "source": [
        "### 3 Training\n",
        "Then start by training your network for 10 epochs using a learning rate of 0.1. You\n",
        "can again copy most of the training procedure from last week. However, you have to\n",
        "change the loss function to fit our binary task! 10. For the beginning try using SGD as\n",
        "an optimizer.\n",
        "\n",
        "Also, when computing the accuracy, slight changes are required to transfer to the\n",
        "binary task. 11\n",
        "\n",
        "\n",
        "10. Have a look at tf.keras.losses.BinaryCrossentropy()\n",
        "11. instead of np.argmax() think about using np.round(,0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "0lV9MjAakqpd",
        "outputId": "cc514064-8800-4c43-c72f-6fea398663d3"
      },
      "source": [
        "def train_step(model, input, target, loss_function, optimizer):\n",
        "  # loss_object and optimizer_object are instances of respective tensorflow classes\n",
        "  with tf.GradientTape() as tape:\n",
        "    prediction = model(input)\n",
        "    loss = loss_function(target, prediction)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  return loss\n",
        "\n",
        "def test(model, test_data, loss_function):\n",
        "  # test over complete test data\n",
        "\n",
        "  test_accuracy_aggregator = []\n",
        "  test_loss_aggregator = []\n",
        "\n",
        "  for (input, target) in test_data:\n",
        "    prediction = model(input)\n",
        "    sample_test_loss = loss_function(target, prediction)\n",
        "\n",
        "     # need to use np.round(,0)\n",
        "    #sample_test_accuracy =  np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
        "    sample_test_accuracy =  np.round(target, 0) == np.round(prediction, 0)\n",
        "    sample_test_accuracy = np.mean(sample_test_accuracy)\n",
        "    test_loss_aggregator.append(sample_test_loss.numpy())\n",
        "    test_accuracy_aggregator.append(np.mean(sample_test_accuracy))\n",
        "\n",
        "  test_loss = tf.reduce_mean(test_loss_aggregator)\n",
        "  test_accuracy = tf.reduce_mean(test_accuracy_aggregator)\n",
        "\n",
        "  return test_loss, test_accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-72efc6169642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#For showcasing we only use a subset of the training and test data (generally use all of the available data!)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCizVH_pkqx9"
      },
      "source": [
        "### 4 Fine-Tuning\n",
        "Now comes the fun part. Your task is to apply at least 3 optimization and regularization\n",
        "techniques featured in the lecture with the goal to significantly increase your model’s\n",
        "performance. 12\n",
        "You can also think about another approach and normalize the input data. 13\n",
        "In the end, everything is up to you! You should witness an increase in performance,\n",
        "stability, and generalization.\n",
        "You may notice that some optimization techniques such as dropout seem to rather\n",
        "decrease performance. However, please bear in mind that these techniques often address\n",
        "ensuring better generalization. So before you start going all out on the test set, don’t\n",
        "forget what the validation set is for.\n",
        "\n",
        "12. If you are using Dropout, think about the flag training= that you can pass onto the call of Dropout.\n",
        "\n",
        "13. For that the stats returned by pandas df.describe() could prove useful to you"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ve667M7k4g9"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#For showcasing we only use a subset of the training and test data (generally use all of the available data!)\n",
        "train_dataset = train_dataset.take(1000)\n",
        "test_dataset = test_dataset.take(100)\n",
        "\n",
        "### Hyperparameters\n",
        "num_epochs = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Initialize the model.\n",
        "model = MyModel()\n",
        "# Initialize the loss: categorical cross entropy. Check out 'tf.keras.losses'.\n",
        "cross_entropy_loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# for binary croos entropy\n",
        "binary_cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "\n",
        "# Initialize the optimizer: SGD with default parameters. Check out 'tf.keras.optimizers'\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "# Initialize lists for later visualization.\n",
        "train_losses = []\n",
        "\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "#testing once before we begin\n",
        "test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
        "test_losses.append(test_loss)\n",
        "test_accuracies.append(test_accuracy)\n",
        "\n",
        "#check how model performs on train data once before we begin\n",
        "train_loss, _ = test(model, train_dataset, cross_entropy_loss)\n",
        "train_losses.append(train_loss)\n",
        "\n",
        "# We train for num_epochs epochs.\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch: {str(epoch)} starting with accuracy {test_accuracies[-1]}')\n",
        "\n",
        "    #training (and checking in with training)\n",
        "    epoch_loss_agg = []\n",
        "    for input,target in train_dataset:\n",
        "        train_loss = train_step(model, input, target, cross_entropy_loss, optimizer)\n",
        "        epoch_loss_agg.append(train_loss)\n",
        "    \n",
        "    #track training loss\n",
        "    train_losses.append(tf.reduce_mean(epoch_loss_agg))\n",
        "\n",
        "    #testing, so we can track accuracy and test loss\n",
        "    test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNT5In_uk4m8"
      },
      "source": [
        "### 5 Visualization\n",
        "Visualize accuracy and loss for training and test data using matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjUxp5ff0Umh"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize accuracy and loss for training and test data.\n",
        "plt.figure()\n",
        "line1, = plt.plot(train_losses)\n",
        "line2, = plt.plot(test_losses)\n",
        "line3, = plt.plot(test_accuracies)\n",
        "plt.xlabel(\"Training steps\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend((line1,line2, line3),(\"training\",\"test\", \"test accuracy\"))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}