{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IANNWTF_hw7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZUT8i_yz9ay9"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.python.keras.layers import Dense"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def integration_task(seq_len, num_samples):\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        noise = np.random.uniform(-2,2, size=(seq_len, 1))\n",
        "        target = [int(np.sum(noise) > 0)]\n",
        "\n",
        "    yield (noise, target)"
      ],
      "metadata": {
        "id": "wqGktq93-WSc"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_integration_task():\n",
        "\n",
        "    # declaring data parameters\n",
        "    num_samples = 50000\n",
        "    seq_len= 30\n",
        "\n",
        "    return integration_task(seq_len, num_samples)"
      ],
      "metadata": {
        "id": "jg1BtCdeGs0K"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the dataset"
      ],
      "metadata": {
        "id": "FG0Nu9x5BoLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing(dataset):\n",
        "\n",
        "    # cache\n",
        "    dataset = dataset.cache()\n",
        "    # shuffle, batch, prefetch our dataset\n",
        "    dataset = dataset.shuffle(5000)\n",
        "    dataset = dataset.batch(32)\n",
        "    dataset = dataset.prefetch(20)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def data_pipeline():\n",
        "\n",
        "    # creating dataset with self-defined generator\n",
        "    dataset = tf.data.Dataset.from_generator(my_integration_task, (tf.float32, tf.int16))\n",
        "\n",
        "    # splitting dataset in training, validation and test data\n",
        "    train_dataset = dataset.take(64000)\n",
        "    remaining = dataset.skip(64000)\n",
        "    valid_dataset = remaining.take(16000)\n",
        "    test_dataset = remaining.skip(16000)\n",
        "\n",
        "    # preprocessing\n",
        "    train_dataset = data_preprocessing(train_dataset)\n",
        "    valid_dataset = data_preprocessing(valid_dataset)\n",
        "    test_dataset = data_preprocessing(test_dataset)\n",
        "\n",
        "    return train_dataset, valid_dataset, test_dataset"
      ],
      "metadata": {
        "id": "zCl7tdeIG4GI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom LSTM Network"
      ],
      "metadata": {
        "id": "qughxIaSI-ZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM Cell implementation"
      ],
      "metadata": {
        "id": "H3n1g6xxJELc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_Cell(tf.keras.layers.Layer):\n",
        "    \n",
        "    def __init__(self, units):\n",
        "        \n",
        "        super(LSTM_Cell, self).__init__()\n",
        "        \n",
        "        self.units = units\n",
        "\n",
        "        self.forget_gate = Dense(self.units, bias_initializer='ones', activation = 'sigmoid')\n",
        "        self.input_gate = Dense(self.units, activation='sigmoid')\n",
        "        self.cell_candidate = Dense(self.units, activation='tanh')\n",
        "        self.output_gate = Dense(self.units, activation='sigmoid')\n",
        "\n",
        "        \n",
        "    def call(self, x, states):\n",
        "\n",
        "        (h_t0, c_t0) = states\n",
        "\n",
        "        # input gate\n",
        "        x_i = self.input_gate(x)\n",
        "        h_i = self.input_gate(h_t0)\n",
        "        i = tf.nn.sigmoid(x_i+h_i)\n",
        "\n",
        "        # forget gate\n",
        "        x_f = self.forget_gate(x)\n",
        "        h_f = self.forget_gate(h_t0)\n",
        "        f = tf.nn.sigmoid(x_f + h_f)\n",
        "\n",
        "        # forget old context/cell info\n",
        "        c_t1 = f * c_t0\n",
        "\n",
        "        # updating cell memory\n",
        "        x_c = self.cell_candidate(x)\n",
        "        h_c = self.cell_candidate(h_t0)\n",
        "        c = tf.nn.tanh(x_c + h_c)\n",
        "\n",
        "        m = c * i\n",
        "        c_t1 = m + c_t1\n",
        "        \n",
        "        # output gate\n",
        "        x_o = self.output_gate(x)\n",
        "        h_o = self.output_gate(h_t0)\n",
        "        our = tf.nn.sigmoid(x_o + h_o)\n",
        "\n",
        "        # hidden output\n",
        "        h_t1 = out * tf.nn.tanh(c_t1)\n",
        "\n",
        "        return (h_t1, c_t1)"
      ],
      "metadata": {
        "id": "BqLvkGsSGw1p"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM Layer implementation"
      ],
      "metadata": {
        "id": "46v5U8GtJOgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_Layer(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, cell):\n",
        "\n",
        "        super(LSTM_Layer, self).__init__()\n",
        "\n",
        "        self.cell = cell\n",
        "        self.cell_units = self.cell.units\n",
        "\n",
        "\n",
        "    def zero_states(self, batch_size):\n",
        "\n",
        "        h = tf.zeros((batch_size, self.cell_units), tf.float32)\n",
        "        c = tf.zeros((batch_size, self.cell_units), tf.float32)\n",
        "        return(h, c)\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "\n",
        "        # sequence length per time-steps\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        # initial states from h and c to 0\n",
        "        states = self.zero_states(x.shape[0])\n",
        "        hidden_states = tf.TensorArray(dtype=tf.float32, size=seq_len)\n",
        "\n",
        "        # interating over timesteps\n",
        "        for timestep in tf.range(seq_len):\n",
        "            x_t = x[:,timestep,:]\n",
        "            states = self.cell(x_t, states)\n",
        "\n",
        "            # only saving the hidden-output not the cell-state\n",
        "            (h, c) = states\n",
        "            hidden_states = hidden_states.write(timestep, h)\n",
        "\n",
        "        # transpose hidden_states accordingly (batch and time steps switched)\n",
        "        outputs = tf.transpose(hidden_states.stack(), [1, 0, 2])\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "ED5w_E3sJOOL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM Model implementation"
      ],
      "metadata": {
        "id": "yU4LbVU8JHOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_Model(tf.keras.layers.Layer):\n",
        "    \n",
        "    def __init__(self, units):\n",
        "        \n",
        "        super(LSTM_Model, self).__init__()\n",
        "\n",
        "        self.units = units\n",
        "\n",
        "        # first layer\n",
        "        self.LSTM_layer = LSTM_Layer(LSTM_Cell(self.units))\n",
        "        # classification\n",
        "        self.out = Dense(1, activation=\"sigmoid\")\n",
        "        \n",
        "\n",
        "    @tf.function\n",
        "    def call(self,x):\n",
        "        x = self.LSTM_layer(x)\n",
        "        x = self.out(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "VqWuQ5izFemw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the network\n",
        "Functions for the training step and testing"
      ],
      "metadata": {
        "id": "GCfzTfMGbsBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model, input, target, loss_function, optimizer):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        prediction = model(input)\n",
        "        # only the prediction from the last timestep\n",
        "        prediction = prediction[:,-1,:]\n",
        "        loss = loss_function(target, prediction)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def test(model, test_data, loss_function):\n",
        "\n",
        "    accuracy_aggregator = []\n",
        "    loss_aggregator = []\n",
        "\n",
        "    for (input, target) in test_data:\n",
        "        prediction = model(input)\n",
        "        prediction = prediction[:,-1,:]\n",
        "        loss = loss_function(target, prediction)\n",
        "        loss_aggregator.append(loss.numpy())\n",
        "\n",
        "        for t, p in zip(target, prediction):\n",
        "            accuracy_aggregator.append(tf.cast(np.round(t.numpy(),0) == np.round(p.numpy(),0), tf.float32))\n",
        "\n",
        "    loss = tf.reduce_mean(loss_aggregator)\n",
        "    accuracy = tf.reduce_mean(accuracy_aggregator)\n",
        "\n",
        "    return loss, accuracy"
      ],
      "metadata": {
        "id": "oGNbpacabfc8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classification(model, optimizer, num_epochs, train_dataset, valid_dataset):\n",
        "\n",
        "    # Testing on the validation dataset once before we begin\n",
        "    valid_loss, valid_accuracy = test(model, valid_dataset, cross_entropy_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    valid_accuracies.append(valid_accuracy)\n",
        "\n",
        "    # Testing on the training dataset once before we begin\n",
        "    train_loss, _ = test(model, train_dataset, cross_entropy_loss)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Training the model for num_epochs epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\n",
        "            f'Epoch: {str(epoch+1)} starting with (validation set) accuracy {valid_accuracies[-1]} and loss {valid_losses[-1]}')\n",
        "\n",
        "        # training and calculating loss\n",
        "        epoch_loss_agg = []\n",
        "\n",
        "        for input, target in train_dataset:\n",
        "            train_loss = train_step(model, input, target, cross_entropy_loss, optimizer)\n",
        "            epoch_loss_agg.append(train_loss)\n",
        "\n",
        "        # tracking the training loss\n",
        "        train_losses.append(tf.reduce_mean(epoch_loss_agg))\n",
        "        print(f'Epoch: {str(epoch+1)} train loss: {train_losses[-1]}')\n",
        "\n",
        "        # testing our model in each epoch to track accuracy and loss on the validation set\n",
        "        valid_loss, valid_accuracy = test(model, valid_dataset, cross_entropy_loss)\n",
        "        valid_losses.append(valid_loss)\n",
        "        valid_accuracies.append(valid_accuracy)\n",
        "\n",
        "    results = [train_losses, valid_losses, valid_accuracies]\n",
        "    \n",
        "    return results"
      ],
      "metadata": {
        "id": "1CiKAmOaRSog"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset = data_pipeline()\n",
        "\n",
        "### Hyperparameters\n",
        "learning_rate = 0.001\n",
        "\n",
        "# initialize the loss: categorical cross entropy\n",
        "cross_entropy_loss = tf.keras.losses.BinaryCrossentropy()\n",
        "# initialize the opimizer: adam\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
        "# initialize the model\n",
        "model = LSTM_Model(25)\n",
        "# initialize lists for later visualization\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "valid_accuracies = []\n",
        "\n",
        "with tf.device('/device:gpu:0'):\n",
        "    # training the model\n",
        "    results = classification(model, optimizer, 10, train_dataset, valid_dataset)\n",
        "    model.summary()\n",
        "\n",
        "    # saving results for visualization\n",
        "    train_losses.append(results[0])\n",
        "    valid_losses.append(results[1])\n",
        "    valid_accuracies.append(results[2])\n",
        "\n",
        "    # testing the trained model\n",
        "    _, test_accuracy = test(trained_model, test_dataset, tf.keras.losses.CategoricalCrossentropy(), False)\n",
        "    print(\"Accuracy (test set):\", test_accuracy)\n",
        "\n",
        "    # visualizing losses and accuracy\n",
        "    visualize(train_losses, valid_losses, valid_accuracies)\n"
      ],
      "metadata": {
        "id": "qZNUsMhTb9jc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "854df96d-0d6a-4fb5-db8f-9225538c3315"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-783459655b4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/device:gpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-870e391d5cf6>\u001b[0m in \u001b[0;36mclassification\u001b[0;34m(model, optimizer, num_epochs, train_dataset, valid_dataset)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Testing on the training dataset once before we begin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-bbcf6932228b>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, test_data, loss_function)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"lstm__model\" (type LSTM_Model).\n\nin user code:\n\n    File \"<ipython-input-17-1f779984d180>\", line 17, in call  *\n        x = self.LSTM_layer(x)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"lstm__layer\" (type LSTM_Layer).\n    \n    in user code:\n    \n        File \"<ipython-input-16-07224ddc2981>\", line 31, in call  *\n            states = self.cell(x_t, states)\n        File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: Exception encountered when calling layer \"lstm__cell\" (type LSTM_Cell).\n        \n        in user code:\n        \n            File \"<ipython-input-30-e70a5baec7c3>\", line 21, in call  *\n                h_i = self.input_gate(h_t0)\n        \n            ValueError: Input 0 of layer dense_1 is incompatible with the layer: expected axis -1 of input shape to have value 1 but received input with shape (1, 25)\n        \n        \n        Call arguments received:\n          • x=tf.Tensor(shape=(1, 1), dtype=float32)\n          • states=('tf.Tensor(shape=(1, 25), dtype=float32)', 'tf.Tensor(shape=(1, 25), dtype=float32)')\n    \n    \n    Call arguments received:\n      • x=tf.Tensor(shape=(1, 30, 1), dtype=float32)\n\n\nCall arguments received:\n  • x=tf.Tensor(shape=(1, 30, 1), dtype=float32)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(train_losses, valid_losses, valid_accuracies):\n",
        "    # Visualize loss and accuracy\n",
        "    plt.figure()\n",
        "\n",
        "    line1, = plt.plot(train_losses)\n",
        "    line2, = plt.plot(valid_losses)\n",
        "    line3, = plt.plot(valid_accuracies)\n",
        "    \n",
        "    plt.legend((line1, line2, line3),(\" train_dataset loss\", \" valid_dataset loss\", \" valid_dataset accuracy\"))\n",
        "    plt.xlabel(\"Training epoch\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8hrXtT6qSqPP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}