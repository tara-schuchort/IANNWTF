{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IANNWTF_hw06.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E4GE-ROZSq9"
      },
      "source": [
        "All necessary imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTp3x5glZFoY"
      },
      "source": [
        "import math\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, Conv2D, GlobalAveragePooling2D, Dense, MaxPool2D, AveragePooling2D, Flatten, Add, Concatenate"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2Il-NJlZWoC"
      },
      "source": [
        "# loading the dataset\n",
        "train_download, test_download = tfds.load(\"cifar10\", split=[\"train\", \"test\"], as_supervised=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndg5tKFYZW5I"
      },
      "source": [
        "Piping the dataset through the pipeline to prepare it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQPopdtiZXBL"
      },
      "source": [
        "def data_pipeline(data):\n",
        "  \"\"\" Describtion here\n",
        "  Args:\n",
        "    data:\n",
        "  Return:\n",
        "    data:\n",
        "  \"\"\"\n",
        "\n",
        "  data = data.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
        "\n",
        "  #normalization, brings image values from range [0, 255] to [-1, 1]\n",
        "  data = data.map(lambda img, target: ((img/128.)-1., target))\n",
        "  #data = data.map(lambda img, target: ((img/255.), target)) alternative?\n",
        "\n",
        "  #create one-hot targets\n",
        "  data = data.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
        "\n",
        "  #cache progress in memory, as there is no need to redo it\n",
        "  data = data.cache()\n",
        "\n",
        "  #shuffle, batch, prefetch\n",
        "  data = data.shuffle(1000)\n",
        "  data = data.batch(64)\n",
        "  data = data.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "  #return preprocessed dataset\n",
        "  return data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lubLSbxcZXHn"
      },
      "source": [
        "# Models\n",
        "\n",
        "ResBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P9TrIcDdfqm"
      },
      "source": [
        "class ResidualBlock(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_filters):\n",
        "\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.batchnorm1 = BatchNormalization()\n",
        "        self.activation1 = Activation(\"relu\")\n",
        "        self.convolution1 = Conv2D(filters=num_filters, kernel_size=(1,1))\n",
        "\n",
        "        self.batchnorm2 = BatchNormalization()\n",
        "        self.activation2 = Activation(\"relu\")\n",
        "        self.convolution2 = Conv2D(filters=num_filters, kernel_size=(3,3), padding=\"same\")\n",
        "\n",
        "        self.batchnorm3 = BatchNormalization()\n",
        "        self.activation3 = Activation(\"relu\")\n",
        "        self.convolution3 = Conv2D(filters=num_filters, kernel_size=(1,1))\n",
        "\n",
        "        self.add = Add()\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs, for_training=None):\n",
        "\n",
        "        x = self.batchnorm1(inputs, training=for_training)\n",
        "        x = self.activation1(x)\n",
        "        x = self.convolution1(x)\n",
        "\n",
        "        x = self.batchnorm2(x, training=for_training)\n",
        "        x = self.activation2(x)\n",
        "        x = self.convolution2(x)\n",
        "\n",
        "        x = self.batchnorm3(x, training=for_training)\n",
        "        x = self.activation3(x)\n",
        "        x = self.convolution3(x)\n",
        "\n",
        "        x = self.add([x, inputs])\n",
        "\n",
        "        return x\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGwjJq7AlxBk"
      },
      "source": [
        "ResNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQTwSl_RlwvR"
      },
      "source": [
        "class ResNet(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_blocks, num_block_filters):\n",
        "\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.convolutional = Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\", padding=\"same\",  input_shape=(32, 32, 3))\n",
        "        # self.bn = BatchNormalization()\n",
        "        # self.pool = MaxPool2D(pool_size = 3,strides = 2)\n",
        "\n",
        "        # residual blocks\n",
        "        self.blocks = []\n",
        "        for index in range(num_blocks):\n",
        "            self.blocks.append(ResidualBlock(num_filters=num_block_filters))\n",
        "\n",
        "        # classification\n",
        "        self.global_pool = GlobalAveragePooling2D()\n",
        "        self.out = Dense(10, kernel_regularizer=\"l1_l2\", activation=\"softmax\")\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs, for_training):\n",
        "        \n",
        "        x = self.convolutional(inputs)\n",
        "        #x = self.bn(x,training=is_training)\n",
        "        #x = tf.nn.relu(x)\n",
        "        #x = self.pool(x)\n",
        "\n",
        "        # residual blocks\n",
        "        for index in range(len(self.blocks)):\n",
        "            x = self.blocks[index](x, training=for_training)\n",
        "\n",
        "        # classification\n",
        "        x = self.global_pool(x)\n",
        "        x = self.out(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeNrMQhImTzK"
      },
      "source": [
        "Transition Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AP4zzW5mVXN"
      },
      "source": [
        "class TransitionLayer(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_filters):\n",
        "\n",
        "        super(TransitionLayer, self).__init__()\n",
        "\n",
        "        self.convolutional = Conv2D(filters=num_filters, kernel_size=(1,1), padding=\"same\")  # or maybe padding = valid?\n",
        "        self.batchnorm = BatchNormalization()\n",
        "        self.activation = Activation(\"relu\")\n",
        "        self.pooling = AveragePooling2D(pool_size = 2, strides=(2,2), padding=\"same\") # maybe leave this out\n",
        "\n",
        "    \n",
        "    @tf.function\n",
        "    def call(self, inputs, for_training=None):\n",
        "\n",
        "        x = self.convolutional(inputs)\n",
        "        x = self.batchnorm(x, training=for_training)\n",
        "        x = self.activation(x)\n",
        "        x = self.pooling(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVphriLEmBTX"
      },
      "source": [
        "DenseBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZvgvaJBmFP1"
      },
      "source": [
        "class DenseBlock(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_filters, new_channels):\n",
        "\n",
        "        super(DenseBlock, self).__init__()\n",
        "\n",
        "        self.batchnorm1 = BatchNormalization()\n",
        "        self.activation1 = Activation(\"relu\")\n",
        "        self.convolution1 = Conv2D(filters=num_filters, kernel_size=(1,1), padding=\"valid\")\n",
        "\n",
        "        self.batchnorm2 = BatchNormalization()\n",
        "        self.activation2 = Activation(\"relu\")\n",
        "        self.convolution2 = Conv2D(filters=new_channels, kernel_size=(3,3), padding=\"same\")\n",
        "\n",
        "        self.concat = Concatenate(axis=-1)\n",
        "\n",
        "    \n",
        "    @tf.function\n",
        "    def call(self, inputs, for_training=None):\n",
        "\n",
        "        x = self.batchnorm1(inputs, training=for_training)\n",
        "        x = self.activation1(x)\n",
        "        x = self.convolution1(x)\n",
        "\n",
        "        x = self.batchnorm2(x, training=for_training)\n",
        "        x = self.activation2(x)\n",
        "        x = self.convolution2(x)\n",
        "\n",
        "        x = self.concat([x, inputs])\n",
        "\n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Aw4_74mmFgo"
      },
      "source": [
        "DenseNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODSSEg8DmTXv"
      },
      "source": [
        "class DenseNet(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_blocks, num_filters, new_channels, growth_rate):\n",
        "\n",
        "        super(DenseNet, self).__init__()\n",
        "        \n",
        "        self.convolutional = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\", input_shape=(32, 32, 3))\n",
        "        self.batchnorm = BatchNormalization()\n",
        "\n",
        "        self.blocks = []\n",
        "        for index in range(num_blocks):\n",
        "            self.blocks.append(DenseBlock(num_filters=num_filters, new_channels=new_channels))\n",
        "            self.blocks.append(TransitionLayer(num_filters=growth_rate*2))\n",
        "        self.blocks.append(DenseBlock(num_filters=num_filters, new_channels=new_channels))\n",
        "\n",
        "        self.global_pool = GlobalAveragePooling2D()\n",
        "        self.out = Dense(10, activation=\"softmax\")\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs, for_training=None):\n",
        "\n",
        "        x = self.convolutional(inputs)\n",
        "        x = self.batchnorm(x, training=for_training)\n",
        "        \n",
        "        # denseblocks\n",
        "        for index in range(len(self.blocks)):\n",
        "            x = self.blocks[index](x, training=for_training)\n",
        "        \n",
        "        # classification\n",
        "        x = self.global_pool(x)\n",
        "        x = self.out(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSYjv5VimVhf"
      },
      "source": [
        "# Training and Testing\n",
        "Implementation of test and training functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfGDTA_enN1B"
      },
      "source": [
        "@tf.function\n",
        "def train_step(model, inputs, target, loss_function, optimizer):\n",
        "    # loss_object and optimizer_object are instances of respective tensorflow classes\n",
        "    with tf.GradientTape() as tape:\n",
        "        prediction = model(inputs, for_training=True)\n",
        "        # model.losses then need to reduce to single value\n",
        "        loss = loss_function(target, prediction)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    \n",
        "    return loss\n",
        "\n",
        "\n",
        "def test(model, test_data, loss_function):\n",
        "    # test over complete test data\n",
        "    test_accuracy_aggregator = np.empty(0)\n",
        "    test_loss_aggregator = np.empty(0)\n",
        "\n",
        "    for (input, target) in test_data:\n",
        "        prediction = model(input, for_training=False)\n",
        "        #print(prediction.shape)\n",
        "        sample_test_loss = loss_function(target, prediction)\n",
        "        sample_test_accuracy =  np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
        "        sample_test_accuracy = np.mean(sample_test_accuracy)\n",
        "        test_loss_aggregator = np.append(test_loss_aggregator, sample_test_loss)\n",
        "        test_accuracy_aggregator = np.append(test_accuracy_aggregator, sample_test_accuracy)\n",
        "    \n",
        "    test_loss = tf.reduce_mean(test_loss_aggregator)\n",
        "    test_accuracy = tf.reduce_mean(test_accuracy_aggregator)\n",
        "\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4iSG6NCnN-s"
      },
      "source": [
        "# Execution and Visualization\n",
        "Execution of training and testing on the models, plotting afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxCil7conRG4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fa08184-2691-4abd-eb0b-c2221ead2c88"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Prepare the data\n",
        "test_dataset = data_pipeline(test_download)\n",
        "train_dataset = data_pipeline(train_download)\n",
        "\n",
        "### Hyperparameters\n",
        "num_epochs = 30\n",
        "learning_rate = 0.001  # tf.constant(0.001, dtype=tf.float32)\n",
        "\n",
        "# Initialize the model\n",
        "dens_model = DenseNet(num_blocks=2, num_filters=128, new_channels=32, growth_rate=32)\n",
        "res_model = ResNet(num_blocks=2, num_block_filters=32)\n",
        "\n",
        "# Initialize the loss function.\n",
        "global_loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "# Initialize the optimizer:\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# Initialize numpy arrays for later visualization\n",
        "train_losses = np.empty(0)\n",
        "test_losses = np.empty(0)\n",
        "test_accuracies = np.empty(0)\n",
        "train_accuracies = np.empty(0)\n",
        "\n",
        "for model in (res_model, dens_model):\n",
        "    # testing once before we begin\n",
        "    test_loss, test_accuracy = test(model, test_dataset, global_loss_function)\n",
        "    test_losses = np.append(test_losses, test_loss)\n",
        "    test_accuracies = np.append(test_accuracies, test_accuracy)\n",
        "\n",
        "    # check how model performs on train data once before we begin\n",
        "    train_loss, _ = test(model, train_dataset, global_loss_function)\n",
        "    train_losses = np.append(train_losses, train_loss)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # We train for num_epochs epochs.\n",
        "    for epoch in range(num_epochs):\n",
        "        # Display accuracy at the beginning of each epoch\n",
        "        print(f'Epoch: {str(epoch)} starting with test accuracy {test_accuracies[-1]}')\n",
        "\n",
        "        # Iterating over the data set and checking in with training\n",
        "        epoch_loss_agg = np.empty(0)\n",
        "        for input, target in train_dataset:\n",
        "            train_loss = train_step(model, input, target, global_loss_function, optimizer)\n",
        "            epoch_loss_agg = np.append(epoch_loss_agg, train_loss)\n",
        "\n",
        "        # Track training loss\n",
        "        train_losses = np.append(train_losses, tf.reduce_mean(epoch_loss_agg))\n",
        "\n",
        "        # Computing train accuracy\n",
        "        _, train_accuracy = test(model, train_dataset, global_loss_function)\n",
        "        train_accuracies = np.append(train_accuracies, train_accuracy)\n",
        "\n",
        "        # Display train accuracy\n",
        "        print(f'Epoch: {str(epoch)} finishing with train accuracy {train_accuracies[-1]}')\n",
        "        print(\" \")\n",
        "\n",
        "        # Computing test loss and accuracy\n",
        "        test_loss, test_accuracy = test(model, test_dataset, global_loss_function)\n",
        "        test_losses = np.append(test_losses, test_loss)\n",
        "        test_accuracies = np.append(test_accuracies, test_accuracy)\n",
        "\n",
        "    # Visualize accuracy and loss for training and test data\n",
        "    plt.figure()\n",
        "    line1, = plt.plot(train_losses)\n",
        "    line2, = plt.plot(test_losses)\n",
        "    line3, = plt.plot(test_accuracies)\n",
        "    line4, = plt.plot(train_accuracies)\n",
        "    plt.xlabel(\"Training steps\")\n",
        "    plt.ylabel(\"Loss/Accuracy\")\n",
        "    plt.legend((line1, line2, line3, line4), (\"training loss\", \"test loss\", \"test accuracy\", \"train accuracy\"))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"res_net\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_9 (Conv2D)           multiple                  896       \n",
            "                                                                 \n",
            " residual_block (ResidualBlo  multiple                 11744     \n",
            " ck)                                                             \n",
            "                                                                 \n",
            " residual_block_1 (ResidualB  multiple                 11744     \n",
            " lock)                                                           \n",
            "                                                                 \n",
            " global_average_pooling2d_1   multiple                 0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  330       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,714\n",
            "Trainable params: 24,330\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "Epoch: 0 starting with test accuracy 0.09922372611464968\n",
            "Epoch: 0 finishing with train accuracy 0.36113331202046034\n",
            " \n",
            "Epoch: 1 starting with test accuracy 0.3642515923566879\n",
            "Epoch: 1 finishing with train accuracy 0.4316656010230179\n",
            " \n",
            "Epoch: 2 starting with test accuracy 0.4286425159235669\n",
            "Epoch: 2 finishing with train accuracy 0.4684702685421995\n",
            " \n",
            "Epoch: 3 starting with test accuracy 0.47014331210191085\n",
            "Epoch: 3 finishing with train accuracy 0.49422554347826086\n",
            " \n",
            "Epoch: 4 starting with test accuracy 0.495421974522293\n"
          ]
        }
      ]
    }
  ]
}