{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUseIaV-VifT"
      },
      "source": [
        "imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKKazT5ZVjBA"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIU4_IqfVoig"
      },
      "source": [
        "dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2bOFzxDVqD8",
        "outputId": "4a3b7f25-5649-4f59-f706-72843319362e"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data(label_mode='fine')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 3s 0us/step\n",
            "169017344/169001437 [==============================] - 3s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVJXzrvJWssq"
      },
      "source": [
        "\n",
        "def prepare_mnist_data(x , y):\n",
        "\n",
        "  mnist = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "\n",
        "  mnist = mnist.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
        "\n",
        "  #sloppy input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
        "  mnist = mnist.map(lambda img, target: ((img/128.)-1., target))\n",
        "\n",
        "  #create one-hot targets\n",
        "  mnist = mnist.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
        "\n",
        "  #cache this progress in memory, as there is no need to redo it; it is deterministic after all\n",
        "  mnist = mnist.cache()\n",
        "\n",
        "\n",
        "  #shuffle, batch, prefetch\n",
        "  mnist = mnist.shuffle(1000)\n",
        "  mnist = mnist.batch(64)\n",
        "  mnist = mnist.prefetch(20)\n",
        "\n",
        "  #return preprocessed dataset\n",
        "  return mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUQCZ6VvX86z",
        "outputId": "8c36c648-6d20-4f17-acc1-9f37caa48b72"
      },
      "source": [
        "train_dataset = prepare_mnist_data(x_train,y_train)\n",
        "test_dataset = prepare_mnist_data(x_test,y_test)\n",
        "\n",
        "print(train_dataset)\n",
        "print(test_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PrefetchDataset shapes: ((None, 32, 32, 3), (None, 1, 10)), types: (tf.float32, tf.float32)>\n",
            "<PrefetchDataset shapes: ((None, 32, 32, 3), (None, 1, 10)), types: (tf.float32, tf.float32)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6-dcx2smoHY"
      },
      "source": [
        "Resnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNkZLIefgNWX"
      },
      "source": [
        "\n",
        "\n",
        "from tensorflow.keras.layers import Dense,BatchNormalization\n",
        "\n",
        "class My_resblock(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, my_kernal, my_filter ):\n",
        "        super(My_resblock, self).__init__()\n",
        "\n",
        "        # target \n",
        "\n",
        "        # conv 1 (3x3) ()32x32x3 ===> (32,32,32)\n",
        "        self.convlayer_1 = tf.keras.layers.Conv2D(filters=my_filter[0], kernel_size=my_kernal[0], strides=(1, 1), padding=\"same\",activation=tf.nn.relu, \n",
        "                                                  kernel_regularizer = tf.keras.regularizers.l2(l2=0.01))\n",
        "        # batch (32,32,32)\n",
        "        self.batchnorm_1 = BatchNormalization()\n",
        "\n",
        "\n",
        "        #conv 2\n",
        "        self.convlayer_2 = tf.keras.layers.Conv2D(filters=my_filter[1], kernel_size=my_kernal[1], strides=(1, 1), padding=\"same\",activation=tf.nn.softmax,\n",
        "                                                  kernel_regularizer = tf.keras.regularizers.l2(l2=0.01))\n",
        "        # batch(32,32,32)\n",
        "        self.batchnorm_2 = BatchNormalization()   \n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "       \n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        \n",
        "\n",
        "        x = self.convlayer_1(inputs)\n",
        "        x = self.batchnorm_1(x)\n",
        "        x = self.convlayer_2(x)\n",
        "        x = self.batchnorm_2(x)\n",
        "\n",
        "        x += inputs\n",
        "         # we have to add it up(32,32,32) + (32,32,3)       \n",
        "\n",
        "        return x\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1jd720SmxbW"
      },
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0w2c9XSmyfK"
      },
      "source": [
        "from tensorflow.keras.layers import Dense,BatchNormalization\n",
        "\n",
        "class My_resnet(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(My_resnet, self).__init__()\n",
        "        \n",
        "\n",
        "        self.convlayer_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1, 1), padding=\"same\",activation=tf.nn.relu, \n",
        "                                                  kernel_regularizer = tf.keras.regularizers.l2(l2=0.01))\n",
        "        self.My_resblock_1 = My_resblock(my_kernal= (1,3), my_filter = (16,32))\n",
        "       \n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        x = self.convlayer_1(inputs)\n",
        "        x = self.My_resblock_1(x)\n",
        "        return x\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8_qZ7EhyFeV"
      },
      "source": [
        "dope = tf.ones((1,5,5,256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxGLSwjcyFsk",
        "outputId": "4a7b2bc8-2c58-4fd1-aee0-f3a29f78708d"
      },
      "source": [
        "my_net = My_resnet()\n",
        "\n",
        "my_net(dope).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 5, 5, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzJGuYEA2KG6"
      },
      "source": [
        "Densenet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBeaJuiK2IvX"
      },
      "source": [
        "from tensorflow.keras.layers import Dense,BatchNormalization\n",
        "\n",
        "class My_denseblock(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, my_kernal=(1,3), my_filter=(16,32) ):\n",
        "        super(My_denseblock, self).__init__()\n",
        "\n",
        "        # target \n",
        "        self.batchnorm_1 = BatchNormalization()\n",
        "\n",
        "        # conv 1 (3x3) ()32x32x3 ===> (32,32,32)\n",
        "        self.convlayer_1 = tf.keras.layers.Conv2D(filters=my_filter[0], kernel_size=my_kernal[0], strides=(1, 1), padding=\"same\",activation=tf.nn.relu, \n",
        "                                                  kernel_regularizer = tf.keras.regularizers.l2(l2=0.01))\n",
        "        # batch (32,32,32)\n",
        "\n",
        "        self.batchnorm_2 = BatchNormalization()\n",
        "        \n",
        "\n",
        "\n",
        "        #conv 2\n",
        "        self.convlayer_2 = tf.keras.layers.Conv2D(filters=my_filter[1], kernel_size=my_kernal[1], strides=(1, 1), padding=\"same\",activation=tf.nn.softmax,\n",
        "                                                  kernel_regularizer = tf.keras.regularizers.l2(l2=0.01))\n",
        "        # batch(32,32,32)\n",
        "        self.batchnorm_3 = BatchNormalization()   \n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "       \n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        \n",
        "\n",
        "        x = self.batchnorm_1(inputs)\n",
        "\n",
        "        x = self.convlayer_1(x)\n",
        "        x = self.batchnorm_2(x)\n",
        "        x = self.convlayer_2(x)\n",
        "        x = self.batchnorm_3(x)\n",
        "\n",
        "        x =tf.keras.layers.Concatenate(axis=3)([x, inputs])\n",
        "         # we have to add it up(32,32,32) + (32,32,3)       \n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjKlX5mm2axB"
      },
      "source": [
        "from tensorflow.keras.layers import Dense,BatchNormalization\n",
        "\n",
        "class My_densenet(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, num_blocks):\n",
        "        super(My_densenet, self).__init__()\n",
        "        \n",
        "\n",
        "        self.convlayer_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1, 1), padding=\"same\",activation=tf.nn.relu, \n",
        "                                                  kernel_regularizer = tf.keras.regularizers.l2(l2=0.01))\n",
        "        \n",
        "        self.blocks = [(My_denseblock(),My_transition()) for _ in range(num_blocks)]\n",
        "        #self.My_densenet_1 = My_denseblock(my_kernal= (1,3), my_filter = (16,32))\n",
        "        self.my_global = tf.keras.layers.GlobalAveragePooling2D()\n",
        "\n",
        "        self.out_put = tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "\n",
        "        \n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        x = self.convlayer_1(inputs)\n",
        "        for dense_block, transition in self.blocks:\n",
        "          x = dense_block(x)\n",
        "          x = transition(x)\n",
        "\n",
        "        x = self.my_global(x)\n",
        "        x = self.out_put(x)\n",
        "\n",
        "        #x = self.blocks(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot1cd9_Y3gYb"
      },
      "source": [
        "Transition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ5hI4GE3fqU"
      },
      "source": [
        "from tensorflow.keras.layers import Dense,BatchNormalization\n",
        "\n",
        "class My_transition(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, filter_number= 32):\n",
        "        super(My_transition, self).__init__()\n",
        "        \n",
        "\n",
        "        self.convlayer_1 = tf.keras.layers.Conv2D(filters=filter_number, kernel_size=(1,1), strides=(1, 1), padding=\"valid\",activation=tf.nn.relu, \n",
        "                                                  kernel_regularizer = tf.keras.regularizers.l2(l2=0.01))\n",
        "        \n",
        "        self.poolinglayer_1 = tf.keras.layers.AvgPool2D(pool_size=(2,2) ,strides=(2, 2), padding=\"valid\")\n",
        "        \n",
        "       \n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        x = self.convlayer_1(inputs)\n",
        "        x = self.poolinglayer_1(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsVeVz1sVMZD"
      },
      "source": [
        "dense = My_densenet(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "id": "WrKTB42XVmVK",
        "outputId": "83cd8a22-e35a-4092-a94e-7c9b769a8518"
      },
      "source": [
        "x = tf.ones((1,5,5,265))\n",
        "dense(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-c620b8eb901d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m265\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"my_densenet_7\" (type My_densenet).\n\nin user code:\n\n    File \"<ipython-input-51-778c45054d38>\", line 24, in call  *\n        x = transition(x)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"my_transition_14\" (type My_transition).\n    \n    in user code:\n    \n        File \"<ipython-input-48-a342c1953219>\", line 19, in call  *\n            x = self.poolinglayer_1(x)\n        File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: Exception encountered when calling layer \"average_pooling2d_14\" (type AveragePooling2D).\n        \n        Negative dimension size caused by subtracting 2 from 1 for '{{node average_pooling2d_14/AvgPool}} = AvgPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](conv2d_71/Relu)' with input shapes: [1,1,1,32].\n        \n        Call arguments received:\n          • inputs=tf.Tensor(shape=(1, 1, 1, 32), dtype=float32)\n    \n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(1, 1, 1, 64), dtype=float32)\n\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(1, 5, 5, 265), dtype=float32)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8KAOSqO6Udz"
      },
      "source": [
        "implementation of tenas and dense blocks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W801SnR06XpJ"
      },
      "source": [
        "def train_step(model, input, target, loss_function, optimizer):\n",
        "  # loss_object and optimizer_object are instances of respective tensorflow classes\n",
        "  with tf.GradientTape() as tape:\n",
        "    prediction = model(input)\n",
        "    #print(prediction)\n",
        "\n",
        "    # model.losses then need to reduce to single value\n",
        "    loss = loss_function(target, prediction)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  return loss\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjrtJ7uoSBn5"
      },
      "source": [
        "def test(model, test_data, loss_function):\n",
        "  # test over complete test data\n",
        "\n",
        "  test_accuracy_aggregator = []\n",
        "  test_loss_aggregator = []\n",
        "\n",
        "  for (input, target) in test_data:\n",
        "    prediction = model(input)\n",
        "    #print(prediction.shape)\n",
        "    sample_test_loss = loss_function(target, prediction)\n",
        "    sample_test_accuracy =  np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
        "    sample_test_accuracy = np.mean(sample_test_accuracy)\n",
        "    test_loss_aggregator.append(sample_test_loss.numpy())\n",
        "    test_accuracy_aggregator.append(np.mean(sample_test_accuracy))\n",
        "\n",
        "  test_loss = tf.reduce_mean(test_loss_aggregator)\n",
        "  test_accuracy = tf.reduce_mean(test_accuracy_aggregator)\n",
        "\n",
        "  return test_loss, test_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M4jfr39SCoq"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#For showcasing we only use a subset of the training and test data (generally use all of the available data!)\n",
        "#train_dataset = train_dataset.take(1000)\n",
        "#test_dataset = test_dataset.take(100)\n",
        "\n",
        "### Hyperparameters\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Initialize the model.\n",
        "model = My_Model()\n",
        "# Initialize the loss: categorical cross entropy. Check out 'tf.keras.losses'.\n",
        "cross_entropy_loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "# Initialize the optimizer: SGD with default parameters. Check out 'tf.keras.optimizers'\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# Initialize lists for later visualization.\n",
        "train_losses = []\n",
        "\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "#testing once before we begin\n",
        "test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
        "test_losses.append(test_loss)\n",
        "test_accuracies.append(test_accuracy)\n",
        "\n",
        "#check how model performs on train data once before we begin\n",
        "train_loss, _ = test(model, train_dataset, cross_entropy_loss)\n",
        "train_losses.append(train_loss)\n",
        "\n",
        "# We train for num_epochs epochs.\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch: {str(epoch)} starting with accuracy {test_accuracies[-1]}')\n",
        "\n",
        "    #training (and checking in with training)\n",
        "    epoch_loss_agg = []\n",
        "    for input,target in train_dataset:\n",
        "        train_loss = train_step(model, input, target, cross_entropy_loss, optimizer)\n",
        "        epoch_loss_agg.append(train_loss)\n",
        "    \n",
        "    #track training loss\n",
        "    train_losses.append(tf.reduce_mean(epoch_loss_agg))\n",
        "\n",
        "    #testing, so we can track accuracy and test loss\n",
        "    test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvxeamZ33gCS"
      },
      "source": [
        ""
      ]
    }
  ]
}