{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUseIaV-VifT"
      },
      "source": [
        "imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKKazT5ZVjBA"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIU4_IqfVoig"
      },
      "source": [
        "dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2bOFzxDVqD8",
        "outputId": "08cfb1cd-e008-42f4-e128-742bd5ca7ae4"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVJXzrvJWssq"
      },
      "source": [
        "\n",
        "def prepare_mnist_data(x , y):\n",
        "\n",
        "  mnist = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "\n",
        "  mnist = mnist.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
        "\n",
        "  #sloppy input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
        "  mnist = mnist.map(lambda img, target: ((img/128.)-1., target))\n",
        "\n",
        "  #create one-hot targets\n",
        "  mnist = mnist.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
        "\n",
        "  #cache this progress in memory, as there is no need to redo it; it is deterministic after all\n",
        "  mnist = mnist.cache()\n",
        "\n",
        "\n",
        "  #shuffle, batch, prefetch\n",
        "  mnist = mnist.shuffle(1000)\n",
        "  mnist = mnist.batch(64)\n",
        "  mnist = mnist.prefetch(20)\n",
        "\n",
        "  #return preprocessed dataset\n",
        "  return mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUQCZ6VvX86z",
        "outputId": "eeba7e90-4839-4e20-e623-dbc5472d1158"
      },
      "source": [
        "train_dataset = prepare_mnist_data(x_train,y_train)\n",
        "test_dataset = prepare_mnist_data(x_test,y_test)\n",
        "\n",
        "print(train_dataset)\n",
        "print(test_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PrefetchDataset shapes: ((None, 32, 32, 3), (None, 1, 10)), types: (tf.float32, tf.float32)>\n",
            "<PrefetchDataset shapes: ((None, 32, 32, 3), (None, 1, 10)), types: (tf.float32, tf.float32)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6-dcx2smoHY"
      },
      "source": [
        "Resnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNkZLIefgNWX"
      },
      "source": [
        "\n",
        "\n",
        "from tensorflow.keras.layers import Dense,BatchNormalization\n",
        "\n",
        "class My_resblock(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, my_kernal, my_filter ):\n",
        "        super(My_resblock, self).__init__()\n",
        "\n",
        "        # target \n",
        "\n",
        "        # conv 1 (3x3) ()32x32x3 ===> (32,32,32)\n",
        "        self.convlayer_1 = tf.keras.layers.Conv2D(filters=my_filter[0], kernel_size=my_kernal[0], strides=(1, 1), padding=\"same\",activation=tf.nn.relu, \n",
        "                                                  kernel_regularizer = tf.keras.regularizers.l2(l2=0.01))\n",
        "        # batch (32,32,32)\n",
        "        self.batchnorm_1 = BatchNormalization()\n",
        "\n",
        "\n",
        "        #conv 2\n",
        "        self.convlayer_2 = tf.keras.layers.Conv2D(filters=my_filter[1], kernel_size=my_kernal[1], strides=(1, 1), padding=\"same\",activation=tf.nn.softmax,\n",
        "                                                  kernel_regularizer = tf.keras.regularizers.l2(l2=0.01))\n",
        "        # batch(32,32,32)\n",
        "        self.batchnorm_2 = BatchNormalization()   \n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "       \n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        \n",
        "\n",
        "        x = self.convlayer_1(inputs)\n",
        "        x = self.batchnorm_1(x)\n",
        "        x = self.convlayer_2(x)\n",
        "        x = self.batchnorm_2(x)\n",
        "\n",
        "        x += inputs\n",
        "         # we have to add it up(32,32,32) + (32,32,3)       \n",
        "\n",
        "        return x\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1jd720SmxbW"
      },
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0w2c9XSmyfK"
      },
      "source": [
        "from tensorflow.keras.layers import Dense,BatchNormalization\n",
        "\n",
        "class My_resnet(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(My_resnet, self).__init__()\n",
        "        \n",
        "\n",
        "        self.convlayer_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1, 1), padding=\"same\",activation=tf.nn.relu, \n",
        "                                                  kernel_regularizer = tf.keras.regularizers.l2(l2=0.01))\n",
        "        self.My_resblock_1 = My_resblock(my_kernal= (1,3), my_filter = (16,32))\n",
        "       \n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        x = self.convlayer_1(inputs)\n",
        "        x = self.My_resblock_1(x)\n",
        "        return x\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8_qZ7EhyFeV"
      },
      "source": [
        "dope = tf.ones((1,5,5,256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxGLSwjcyFsk",
        "outputId": "4a7b2bc8-2c58-4fd1-aee0-f3a29f78708d"
      },
      "source": [
        "my_net = My_resnet()\n",
        "\n",
        "my_net(dope).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 5, 5, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzJGuYEA2KG6"
      },
      "source": [
        "Densenet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBeaJuiK2IvX"
      },
      "source": [
        "from tensorflow.keras.layers import Dense,BatchNormalization\n",
        "\n",
        "class My_denseblock(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, my_kernal=(1,3), my_filter=(16,32) ):\n",
        "        super(My_denseblock, self).__init__()\n",
        "\n",
        "        # target \n",
        "        self.batchnorm_1 = BatchNormalization()\n",
        "\n",
        "        # conv 1 (3x3) ()32x32x3 ===> (32,32,32)\n",
        "        self.convlayer_1 = tf.keras.layers.Conv2D(filters=my_filter[0], kernel_size=my_kernal[0], strides=(1, 1), padding=\"same\",activation=tf.nn.relu, \n",
        "                                                  kernel_regularizer = tf.keras.regularizers.l2(l2=0.01))\n",
        "        # batch (32,32,32)\n",
        "\n",
        "        self.batchnorm_2 = BatchNormalization()\n",
        "        \n",
        "\n",
        "\n",
        "        #conv 2\n",
        "        self.convlayer_2 = tf.keras.layers.Conv2D(filters=my_filter[1], kernel_size=my_kernal[1], strides=(1, 1), padding=\"same\",activation=tf.nn.softmax,\n",
        "                                                  kernel_regularizer = tf.keras.regularizers.l2(l2=0.01))\n",
        "        # batch(32,32,32)\n",
        "        self.batchnorm_3 = BatchNormalization()   \n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "       \n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        \n",
        "\n",
        "        x = self.batchnorm_1(inputs)\n",
        "\n",
        "        x = self.convlayer_1(x)\n",
        "        x = self.batchnorm_2(x)\n",
        "        x = self.convlayer_2(x)\n",
        "        x = self.batchnorm_3(x)\n",
        "\n",
        "        x =tf.keras.layers.Concatenate(axis=3)([x, inputs])\n",
        "         # we have to add it up(32,32,32) + (32,32,3)       \n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjKlX5mm2axB"
      },
      "source": [
        "from tensorflow.keras.layers import Dense,BatchNormalization\n",
        "\n",
        "class My_densenet(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, num_blocks ):\n",
        "        super(My_densenet, self).__init__()\n",
        "        \n",
        "\n",
        "        self.convlayer_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1, 1), padding=\"same\",activation=tf.nn.relu, \n",
        "                                                  kernel_regularizer = tf.keras.regularizers.l2(l2=0.01))\n",
        "        \n",
        "        self.blocks = [(My_denseblock(),My_transition()) for _ in range(num_blocks)]\n",
        "        #self.My_densenet_1 = My_denseblock(my_kernal= (1,3), my_filter = (16,32))\n",
        "        self.my_global = tf.keras.layers.GlobalAveragePooling2D() # for flatten vector , for  each feature map we get one value\n",
        "\n",
        "        self.out_put = tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "\n",
        "        \n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        x = self.convlayer_1(inputs)\n",
        "        for dense_block, transition in self.blocks:\n",
        "          x = dense_block(x)\n",
        "          x = transition(x)\n",
        "\n",
        "        x = self.my_global(x)\n",
        "        x = self.out_put(x)\n",
        "\n",
        "        #x = self.blocks(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot1cd9_Y3gYb"
      },
      "source": [
        "Transition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ5hI4GE3fqU"
      },
      "source": [
        "from tensorflow.keras.layers import Dense,BatchNormalization\n",
        "\n",
        "class My_transition(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, filter_number= 16):\n",
        "        super(My_transition, self).__init__()\n",
        "        \n",
        "\n",
        "        self.convlayer_1 = tf.keras.layers.Conv2D(filters=filter_number, kernel_size=(1,1), strides=(1, 1), padding=\"same\",activation=tf.nn.relu, \n",
        "                                                  kernel_regularizer = tf.keras.regularizers.l2(l2=0.01))\n",
        "        \n",
        "        self.poolinglayer_1 = tf.keras.layers.AvgPool2D(pool_size=(2,2) ,strides=(2, 2), padding=\"valid\")\n",
        "        \n",
        "       \n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        x = self.convlayer_1(inputs)\n",
        "        x = self.poolinglayer_1(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsVeVz1sVMZD"
      },
      "source": [
        "dense = My_densenet(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrKTB42XVmVK",
        "outputId": "02bf8caa-bcb7-4d77-e8bc-28788267fa74"
      },
      "source": [
        "x = tf.ones((1,32,32,265))\n",
        "dense(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
              "array([[0.10231405, 0.09096028, 0.09671639, 0.10703692, 0.0982039 ,\n",
              "        0.10491148, 0.10098356, 0.10856821, 0.09110335, 0.09920186]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8KAOSqO6Udz"
      },
      "source": [
        "implementation of tenas and dense blocks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W801SnR06XpJ"
      },
      "source": [
        "def train_step(model, input, target, loss_function, optimizer):\n",
        "  # loss_object and optimizer_object are instances of respective tensorflow classes\n",
        "  with tf.GradientTape() as tape:\n",
        "    prediction = model(input)\n",
        "    #print(prediction.shape)\n",
        "   # prediction = tf.squeeze(prediction) # for matching the shape of predictuihn adn target\n",
        "    #print(prediction)\n",
        "\n",
        "    # model.losses then need to reduce to single value\n",
        "    loss = loss_function(tf.squeeze(target), prediction)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  return loss\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjrtJ7uoSBn5"
      },
      "source": [
        "def test(model, test_data, loss_function):\n",
        "  # test over complete test data\n",
        "\n",
        "  test_accuracy_aggregator = []\n",
        "  test_loss_aggregator = []\n",
        "\n",
        "  for (input, target) in test_data:\n",
        "    prediction = model(input)\n",
        "    #print(prediction.shape)\n",
        "    sample_test_loss = loss_function(tf.squeeze(target), prediction)\n",
        "    \n",
        "    #print(target.shape, prediction.shape)\n",
        "    #break\n",
        "    sample_test_accuracy =  np.argmax(tf.squeeze(target), axis=1) == np.argmax(prediction, axis=1)\n",
        "    sample_test_accuracy = np.mean(sample_test_accuracy)\n",
        "    test_loss_aggregator.append(sample_test_loss.numpy())\n",
        "    test_accuracy_aggregator.append(np.mean(sample_test_accuracy))\n",
        "\n",
        "  test_loss = tf.reduce_mean(test_loss_aggregator)\n",
        "  test_accuracy = tf.reduce_mean(test_accuracy_aggregator)\n",
        "\n",
        "  return test_loss, test_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PGVi1dvoRYL"
      },
      "source": [
        "model = My_densenet(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11mvPR8Xol4I"
      },
      "source": [
        "demo = tf.ones((1,32,32,3))\n",
        "#model(demo).summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M4jfr39SCoq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51e1a3c4-2bdc-4ff1-b4ea-16535027eebd"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#For showcasing we only use a subset of the training and test data (generally use all of the available data!)\n",
        "#train_dataset = train_dataset.take(1000)\n",
        "#test_dataset = test_dataset.take(100)\n",
        "\n",
        "### Hyperparameters\n",
        "num_epochs = 30\n",
        "learning_rate = 0.01\n",
        "model = My_densenet(4)\n",
        "# Initialize the model.\n",
        "#model = My_Model()\n",
        "\n",
        "# Initialize the loss: categorical cross entropy. Check out 'tf.keras.losses'.\n",
        "cross_entropy_loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "# Initialize the optimizer: SGD with default parameters. Check out 'tf.keras.optimizers'\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# Initialize lists for later visualization.\n",
        "train_losses = []\n",
        "\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "#testing once before we begin\n",
        "test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
        "test_losses.append(test_loss)\n",
        "test_accuracies.append(test_accuracy)\n",
        "\n",
        "#check how model performs on train data once before we begin\n",
        "train_loss, _ = test(model, train_dataset, cross_entropy_loss)\n",
        "train_losses.append(train_loss)\n",
        "\n",
        "# We train for num_epochs epochs.\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch: {str(epoch)} starting with accuracy {test_accuracies[-1]}')\n",
        "\n",
        "    #training (and checking in with training)\n",
        "    epoch_loss_agg = []\n",
        "    for input,target in train_dataset:\n",
        "        train_loss = train_step(model, input, target, cross_entropy_loss, optimizer)\n",
        "        epoch_loss_agg.append(train_loss)\n",
        "    \n",
        "    #track training loss\n",
        "    train_losses.append(tf.reduce_mean(epoch_loss_agg))\n",
        "\n",
        "    #testing, so we can track accuracy and test loss\n",
        "    test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 starting with accuracy 0.10061703821656051\n",
            "Epoch: 1 starting with accuracy 0.32902070063694266\n",
            "Epoch: 2 starting with accuracy 0.3786823248407643\n",
            "Epoch: 3 starting with accuracy 0.4212778662420382\n",
            "Epoch: 4 starting with accuracy 0.4380971337579618\n",
            "Epoch: 5 starting with accuracy 0.4238654458598726\n",
            "Epoch: 6 starting with accuracy 0.4590963375796178\n",
            "Epoch: 7 starting with accuracy 0.4667595541401274\n",
            "Epoch: 8 starting with accuracy 0.45660828025477707\n",
            "Epoch: 9 starting with accuracy 0.48865445859872614\n",
            "Epoch: 10 starting with accuracy 0.47999601910828027\n",
            "Epoch: 11 starting with accuracy 0.5008957006369427\n",
            "Epoch: 12 starting with accuracy 0.5119426751592356\n",
            "Epoch: 13 starting with accuracy 0.498109076433121\n",
            "Epoch: 14 starting with accuracy 0.5215963375796179\n",
            "Epoch: 15 starting with accuracy 0.5252786624203821\n",
            "Epoch: 16 starting with accuracy 0.5100517515923567\n",
            "Epoch: 17 starting with accuracy 0.5140326433121019\n",
            "Epoch: 18 starting with accuracy 0.5184116242038217\n",
            "Epoch: 19 starting with accuracy 0.5188097133757962\n",
            "Epoch: 20 starting with accuracy 0.5460788216560509\n",
            "Epoch: 21 starting with accuracy 0.5390127388535032\n",
            "Epoch: 22 starting with accuracy 0.5475716560509554\n",
            "Epoch: 23 starting with accuracy 0.5699641719745223\n",
            "Epoch: 24 starting with accuracy 0.5638933121019108\n",
            "Epoch: 25 starting with accuracy 0.544984076433121\n",
            "Epoch: 26 starting with accuracy 0.5489649681528662\n",
            "Epoch: 27 starting with accuracy 0.5659832802547771\n",
            "Epoch: 28 starting with accuracy 0.567078025477707\n",
            "Epoch: 29 starting with accuracy 0.5772292993630573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7ZZDFD7mtd6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize accuracy and loss for training and test data.\n",
        "plt.figure()\n",
        "line1, = plt.plot(train_losses)\n",
        "line2, = plt.plot(test_losses)\n",
        "line3, = plt.plot(test_accuracies)\n",
        "plt.xlabel(\"Training steps\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend((line1,line2, line3),(\"training\",\"test\", \"test accuracy\"))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvxeamZ33gCS"
      },
      "source": [
        ""
      ]
    }
  ]
}