{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IANNWTF_09.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Necessary imports"
      ],
      "metadata": {
        "id": "F98heQce0505"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from re import split\n",
        "import os\n",
        "import urllib\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "from tensorflow.keras.layers import BatchNormalization, Reshape, Conv2DTranspose, Flatten, Conv2D, Dense, Dropout"
      ],
      "metadata": {
        "id": "sV3AbxyUxV9h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the data"
      ],
      "metadata": {
        "id": "d4CGPKyg02TQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the dataset\n",
        "def load_data():\n",
        "    \"\"\"\n",
        "    Loading the data set directly from google\n",
        "        Returns:\n",
        "            data: processed dataset\n",
        "    \"\"\"\n",
        "\n",
        "    # load images restricted to one category (candles)\n",
        "    categories = [line.rstrip(b'\\n') for line in urllib.request.urlopen('https://raw.githubusercontent.com/googlecreativelab/quickdraw-dataset/master/categories.txt')]\n",
        "    category = 'candle'\n",
        "\n",
        "    # Creates a folder to download the original drawings into.\n",
        "    if not os.path.isdir('npy_files'):\n",
        "        os.mkdir('npy_files')\n",
        "\n",
        "    url = f'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/{category}.npy'\n",
        "    urllib.request.urlretrieve(url, f'npy_files/{category}.npy')\n",
        "\n",
        "    images = np.load(f'npy_files/{category}.npy')\n",
        "    print(f'{len(images)} images to train on')\n",
        "\n",
        "    train_imgs = images[:30000]\n",
        "    valid_imgs = images[30000:40000]\n",
        "    test_imgs = images[40000:50000]\n",
        "\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices(train_imgs)\n",
        "    valid_ds = tf.data.Dataset.from_tensor_slices(valid_imgs)\n",
        "    test_ds = tf.data.Dataset.from_tensor_slices(test_imgs)\n",
        "    \n",
        "    # performing preprocessing steps\n",
        "    train_ds = data_pipeline(train_ds)\n",
        "    valid_ds = data_pipeline(valid_ds)\n",
        "    test_ds = data_pipeline(test_ds)\n",
        "\n",
        "    return train_ds, valid_ds, test_ds\n",
        "\n",
        "def data_pipeline(data):\n",
        "    \"\"\" Describtion here\n",
        "    Args:\n",
        "        data:\n",
        "    Return:\n",
        "        data:\n",
        "    \"\"\"\n",
        "    # casting and reshaping\n",
        "    data = data.map(lambda image: (tf.cast(image, tf.float32)))\n",
        "    data = data.map(lambda image: (tf.reshape(image,[28,28,1])))\n",
        "    # normalization, brings image values from range [0, 255] to [-1, 1]\n",
        "    data = data.map(lambda image: ((image/128)-1))\n",
        "\n",
        "    #cache progress in memory, as there is no need to redo it\n",
        "    data = data.cache()\n",
        "\n",
        "    #shuffle, batch, prefetch\n",
        "    data = data.shuffle(2000)\n",
        "    data = data.batch(64)\n",
        "    data = data.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "F3QbQpLYw5P3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAN Model"
      ],
      "metadata": {
        "id": "Vj9gcKcd1StC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discriminator"
      ],
      "metadata": {
        "id": "sq1EtK0X1ddj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Descriminator(tf.keras.Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Descriminator, self).__init__()\n",
        "\n",
        "        self._conv1 = Conv2D(filters=64, kernel_size=5, strides=2, padding=\"same\")\n",
        "        self._drop1 = Dropout(0.3)\n",
        "\n",
        "        self._conv2 = Conv2D(filters=128, kernel_size=5, strides=2, padding=\"same\")\n",
        "        self._drop2 = Dropout(0.3)\n",
        "\n",
        "        self._flatten = Flatten()\n",
        "        self._dense = Dense(1)\n",
        "\n",
        "    def call(self, x, for_training=None):\n",
        "\n",
        "        x = self._conv1(x, training=for_training)\n",
        "        x = tf.nn.leaky_relu(x)\n",
        "        x = self._drop1(x, training=for_training)\n",
        "\n",
        "        x = self._conv2(x, training=for_training)\n",
        "        x = tf.nn.leaky_relu(x)\n",
        "        x = self._drop2(x, training=for_training)\n",
        "\n",
        "        x = self._flatten(x, training=for_training)\n",
        "        x = self._dense(x, training=for_training)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "S9iVat8A1h0B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generator"
      ],
      "metadata": {
        "id": "juOaqli21fkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(tf.keras.Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self._dense = Dense(7*7*8)\n",
        "\n",
        "        self._bn1 = BatchNormalization()\n",
        "        self._convt1 = Conv2DTranspose(filters=128, kernel_size=5, strides=1, padding=\"same\")\n",
        "\n",
        "        self._bn2 = BatchNormalization()\n",
        "        self._convt2 = Conv2DTranspose(filters=64, kernel_size=5, strides=2, padding=\"same\")\n",
        "\n",
        "        self._bn3 = BatchNormalization()\n",
        "        self._out = Conv2DTranspose(filters=1, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\")\n",
        "\n",
        "    def call(self, x, for_training=None):\n",
        "        x = self._dense(x, training=for_training)\n",
        "\n",
        "        x = self._bn1(x, training=for_training)\n",
        "        x = tf.nn.leaky_relu(x)\n",
        "        x = tf.reshape(x, (64,7,7,8))\n",
        "        x = self._convt1(x, training=for_training)\n",
        "\n",
        "        x = self._bn2(x, training=for_training)\n",
        "        x = tf.nn.leaky_relu(x)\n",
        "        x = self._convt2(x, training=for_training)\n",
        "        \n",
        "        x = self._bn3(x, training=for_training)\n",
        "        x = tf.nn.leaky_relu(x)\n",
        "\n",
        "        x = self._out(x, training=for_training)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "UQumN6U21BLh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "Bq-MZlpZ13Pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(g, d, imgs, loss_function, g_optim, d_optim, for_training):\n",
        "    # generates a random image\n",
        "    noise = tf.random.normal([64,100])\n",
        "\n",
        "    with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
        "\n",
        "        # generate noise vector\n",
        "        generated_imgs = g(noise, training=for_training)\n",
        "\n",
        "        real_out = d(imgs, training=for_training)\n",
        "        fake_out = d(generated_imgs, training=for_training)\n",
        "\n",
        "        # calculating losses\n",
        "        g_loss = loss_function(tf.ones_like(fake_out), fake_out)\n",
        "        d_loss = loss_function(tf.ones_like(real_out)*0.9, real_out) + loss_function(tf.zeros_like(fake_out), fake_out)\n",
        "\n",
        "        # calculaing the gradients\n",
        "        gradients_g = g_tape.gradient(g_loss, g.trainable_variables)\n",
        "        gradients_d = d_tape.gradient(d_loss, d.trainable_variables)\n",
        "\n",
        "    # updating weights and biases\n",
        "    g_optim.apply_gradients(zip(gradients_g, g.trainable_variables))\n",
        "    d_optim.apply_gradients(zip(gradients_d, d.trainable_variables))\n",
        "\n",
        "    return g_loss, d_loss\n",
        "\n",
        "\n",
        "def test(gen, disc, test_data, loss_function, for_training):\n",
        "    # initializing lists for accuracys and loss\n",
        "    accuracy_aggregator = []\n",
        "    g_loss_aggregator = []\n",
        "    d_loss_aggregator = []\n",
        "\n",
        "    for imgs in test_data:\n",
        "        # forward step\n",
        "        noise = tf.random.normal([64,100])\n",
        "        generated_imgs = g(noise, training=for_training)\n",
        "        real_out = d(imgs, training=for_training)\n",
        "        fake_out = d(generated_imgs, training=for_training)\n",
        "\n",
        "        # calculating loss\n",
        "        g_loss = loss_function(tf.ones_like(fake_out), fake_out)\n",
        "        d_loss = loss_function(tf.ones_like(real_out)*0.9, real_out) + loss_function(tf.zeros_like(fake_out)*0.95, fake_out)\n",
        "\n",
        "        # add loss and accuracy to the lists\n",
        "        g_loss_aggregator.append(g_loss.numpy())\n",
        "        d_loss_aggregator.append(d_loss.numpy())\n",
        "\n",
        "    # calculate the mean of the loss and accuracy (for this epoch)\n",
        "    g_loss = tf.reduce_mean(g_loss_aggregator)\n",
        "    d_loss = tf.reduce_mean(d_loss_aggregator)\n",
        "\n",
        "    return g_loss, d_loss\n"
      ],
      "metadata": {
        "id": "3RYnG5fW15wJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classification(g, d, num_epochs, train_ds, valid_ds):\n",
        "    seed = tf.random.normal([64,100])\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    # initialize the loss: categorical cross entropy\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    lr = 0.0002\n",
        "\n",
        "    # optimizers for both models\n",
        "    g_optim = tf.keras.optimizers.Adam(lr,beta_1=0.5)\n",
        "    d_optim = tf.keras.optimizers.Adam(lr,beta_1=0.5)\n",
        "\n",
        "    # initialize lists for later visualization.\n",
        "    train_g_losses = []\n",
        "    valid_g_losses = []\n",
        "    train_d_losses = []\n",
        "    valid_d_losses = []\n",
        "\n",
        "    # testing on our valid_ds once before we begin\n",
        "    valid_g_loss, valid_d_loss = test(g, d, valid_ds, loss, for_training=False)\n",
        "    valid_g_losses.append(valid_g_loss)\n",
        "    valid_d_losses.append(valid_d_loss)\n",
        "\n",
        "    # Testing on our train_ds once before we begin\n",
        "    train_g_loss, train_d_loss = test(g, d, train_ds, loss, for_training=False)\n",
        "    train_g_losses.append(train_g_loss)\n",
        "    train_d_losses.append(train_d_loss)\n",
        "\n",
        "    # training our model for num_epochs epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f' starting with (validation set): g_loss {valid_g_losses[-1]} and d_loss {valid_d_losses[-1]}')\n",
        "        print(f' and (training set): g_loss {train_g_losses[-1]} and d_loss {train_d_losses[-1]}')\n",
        "        print(\"{}/{} epoches\".format(epoch, num_epochs))\n",
        "        \n",
        "        # training (and calculating loss while training)\n",
        "        epoch_g_loss_agg = []\n",
        "        epoch_d_loss_agg = []\n",
        "\n",
        "        for imgs in train_ds:\n",
        "            train_g_loss, train_d_loss = train_step(g, d, imgs, loss, g_optim, d_optim, for_training=True)\n",
        "            epoch_g_loss_agg.append(train_g_loss)\n",
        "            epoch_d_loss_agg.append(train_d_loss)\n",
        "\n",
        "        # track training loss\n",
        "        train_g_losses.append(tf.reduce_mean(epoch_g_loss_agg))\n",
        "        train_d_losses.append(tf.reduce_mean(epoch_d_loss_agg))\n",
        "\n",
        "        ## After i-th epoch plot image\n",
        "        if (epoch % 5) == 0:\n",
        "            fake_image = tf.reshape(g(seed, for_training=False), shape = (64,28,28))\n",
        "            plt.imshow(fake_image[10], cmap = \"gray\")\n",
        "            plt.show()\n",
        "\n",
        "        # testing our model in each epoch to track accuracy and loss on the validation set\n",
        "        valid_g_loss, valid_d_loss = test(g, d, valid_ds, loss, for_training=False)\n",
        "        valid_g_losses.append(valid_g_loss)\n",
        "        valid_d_losses.append(valid_d_loss)\n",
        "\n",
        "    results = [train_g_losses, valid_g_losses, train_d_losses, valid_d_losses]\n",
        "    return results, g, d"
      ],
      "metadata": {
        "id": "9yhtsKXs3HjX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_results(train_g_losses, valid_g_losses, train_d_losses, valid_d_losses):\n",
        "\n",
        "    fig, axs = plt.subplots(2, 1)\n",
        "    #fig.set_size_inches(13, 6)\n",
        "    # making a grid with subplots\n",
        "    for j in range(1):\n",
        "        axs[0].plot(train_g_l[j])\n",
        "        axs[0].plot(valid_g_l[j])\n",
        "        axs[1].plot(train_d_l[j])\n",
        "        axs[1].plot(valid_d_l[j])\n",
        "        axs[1].sharex(axs[0])\n",
        "\n",
        "    fig.legend([\" train_g_l\",\" valid_g_l\",\" train_d_l\",\" valid_d_l\"],loc=\"lower right\")\n",
        "    plt.xlabel(\"Training epoch\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "CyQUQOb33P6W"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "train_ds, valid_ds, test_ds = load_data()\n",
        "\n",
        "d = Descriminator()\n",
        "g = Generator()\n",
        "\n",
        "train_g_losses = []\n",
        "valid_g_losses = []\n",
        "train_d_losses = []\n",
        "valid_d_losses = []\n",
        "\n",
        "\n",
        "with tf.device('/device:gpu:0'):\n",
        "    # training the model\n",
        "    results, trained_g, trained_d = classification(g, d, 10, train_ds, valid_ds)\n",
        "\n",
        "    # saving results for visualization\n",
        "    train_g_losses.append(results[0])\n",
        "    valid_g_losses.append(results[1])\n",
        "    train_d_losses.append(results[2])\n",
        "    valid_d_losses.append(results[3])\n",
        "\n",
        "    # testing the trained model\n",
        "    _, test_accuracy = test(trained_model, test_ds, tf.keras.losses.BinaryCrossentropy(), False)\n",
        "    print(\"Accuracy (test set):\", test_accuracy)\n",
        "\n",
        "# visualizing losses and accuracy\n",
        "visualize_results(train_g_losses, valid_g_losses, train_d_losses, valid_d_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "N-5mqwwtPnpJ",
        "outputId": "9e6ccdb0-202a-4dbd-c311-97c603e7841d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "141545 images to train on\n",
            " starting with (validation set): g_loss 0.6924867033958435 and d_loss 1.3940385580062866\n",
            " and (training set): g_loss 0.6924607157707214 and d_loss 1.3942387104034424\n",
            "0/10 epoches\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " starting with (validation set): g_loss 1.070691466331482 and d_loss 1.086503505706787\n",
            " and (training set): g_loss 0.9579606652259827 and d_loss 1.196457028388977\n",
            "1/10 epoches\n",
            " starting with (validation set): g_loss 1.1470497846603394 and d_loss 1.0514775514602661\n",
            " and (training set): g_loss 1.0479917526245117 and d_loss 1.1837100982666016\n",
            "2/10 epoches\n"
          ]
        }
      ]
    }
  ]
}