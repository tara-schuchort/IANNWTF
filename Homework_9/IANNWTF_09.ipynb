{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IANNWTF_09.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Necessary imports"
      ],
      "metadata": {
        "id": "F98heQce0505"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from re import split\n",
        "import os\n",
        "import urllib\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "from tensorflow.keras.layers import BatchNormalization, Reshape, Conv2DTranspose, Flatten, Conv2D, Dense, Dropout"
      ],
      "metadata": {
        "id": "sV3AbxyUxV9h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the data"
      ],
      "metadata": {
        "id": "d4CGPKyg02TQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the dataset\n",
        "def load_data():\n",
        "    \"\"\"\n",
        "    Loading the data set directly from google\n",
        "        Returns:\n",
        "            data: processed dataset\n",
        "    \"\"\"\n",
        "\n",
        "    # load images restricted to one category (candles)\n",
        "    categories = [line.rstrip(b'\\n') for line in urllib.request.urlopen('https://raw.githubusercontent.com/googlecreativelab/quickdraw-dataset/master/categories.txt')]\n",
        "    category = 'candle'\n",
        "\n",
        "    # Creates a folder to download the original drawings into.\n",
        "    if not os.path.isdir('npy_files'):\n",
        "        os.mkdir('npy_files')\n",
        "\n",
        "    url = f'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/{category}.npy'\n",
        "    urllib.request.urlretrieve(url, f'npy_files/{category}.npy')\n",
        "\n",
        "    images = np.load(f'npy_files/{category}.npy')\n",
        "    print(f'{len(images)} images to train on')\n",
        "\n",
        "    train_imgs = images[:30000]\n",
        "    valid_imgs = images[30000:40000]\n",
        "    test_imgs = images[40000:50000]\n",
        "\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices(train_imgs)\n",
        "    valid_ds = tf.data.Dataset.from_tensor_slices(valid_imgs)\n",
        "    test_ds = tf.data.Dataset.from_tensor_slices(test_imgs)\n",
        "    \n",
        "    # performing preprocessing steps\n",
        "    train_ds = data_pipeline(train_ds)\n",
        "    valid_ds = data_pipeline(valid_ds)\n",
        "    test_ds = data_pipeline(test_ds)\n",
        "\n",
        "    return train_ds, valid_ds, test_ds\n",
        "\n",
        "def data_pipeline(data):\n",
        "    \"\"\" Describtion here\n",
        "    Args:\n",
        "        data:\n",
        "    Return:\n",
        "        data:\n",
        "    \"\"\"\n",
        "    # casting and reshaping\n",
        "    data = data.map(lambda image: (tf.cast(image, tf.float32)))\n",
        "    data = data.map(lambda image: (tf.reshape(image,[28,28,1])))\n",
        "    # normalization, brings image values from range [0, 255] to [-1, 1]\n",
        "    data = data.map(lambda image: ((image/128)-1))\n",
        "\n",
        "    #cache progress in memory, as there is no need to redo it\n",
        "    data = data.cache()\n",
        "\n",
        "    #shuffle, batch, prefetch\n",
        "    data = data.shuffle(2000)\n",
        "    data = data.batch(64)\n",
        "    data = data.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "F3QbQpLYw5P3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAN Model"
      ],
      "metadata": {
        "id": "Vj9gcKcd1StC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discriminator"
      ],
      "metadata": {
        "id": "sq1EtK0X1ddj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Descriminator(tf.keras.Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Descriminator, self).__init__()\n",
        "\n",
        "        self._conv1 = Conv2D(filters=64, kernel_size=5, strides=2, padding=\"same\")\n",
        "        self._drop1 = Dropout(0.3)\n",
        "\n",
        "        self._conv2 = Conv2D(filters=128, kernel_size=5, strides=2, padding=\"same\")\n",
        "        self._drop2 = Dropout(0.3)\n",
        "\n",
        "        self._flatten = Flatten()\n",
        "        self._dense = Dense(1)\n",
        "\n",
        "    def call(self, x, for_training=None):\n",
        "\n",
        "        x = self._conv1(x, training=for_training)\n",
        "        x = tf.nn.leaky_relu(x)\n",
        "        x = self._drop1(x, training=for_training)\n",
        "\n",
        "        x = self._conv2(x, training=for_training)\n",
        "        x = tf.nn.leaky_relu(x)\n",
        "        x = self._drop2(x, training=for_training)\n",
        "\n",
        "        x = self._flatten(x, training=for_training)\n",
        "        x = self._dense(x, training=for_training)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "S9iVat8A1h0B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generator"
      ],
      "metadata": {
        "id": "juOaqli21fkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(tf.keras.Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self._dense = Dense(7*7*8)\n",
        "\n",
        "        self._bn1 = BatchNormalization()\n",
        "        self._convt1 = Conv2DTranspose(filters=128, kernel_size=5, strides=1, padding=\"same\")\n",
        "\n",
        "        self._bn2 = BatchNormalization()\n",
        "        self._convt2 = Conv2DTranspose(filters=64, kernel_size=5, strides=2, padding=\"same\")\n",
        "\n",
        "        self._bn3 = BatchNormalization()\n",
        "        self._out = Conv2DTranspose(filters=1, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\")\n",
        "\n",
        "    def call(self, x, for_training=None):\n",
        "        x = self._dense(x, training=for_training)\n",
        "\n",
        "        x = self._bn1(x, training=for_training)\n",
        "        x = tf.nn.leaky_relu(x)\n",
        "        x = tf.reshape(x, (64,7,7,8))\n",
        "        x = self._convt1(x, training=for_training)\n",
        "\n",
        "        x = self._bn2(x, training=for_training)\n",
        "        x = tf.nn.leaky_relu(x)\n",
        "        x = self._convt2(x, training=for_training)\n",
        "        \n",
        "        x = self._bn3(x, training=for_training)\n",
        "        x = tf.nn.leaky_relu(x)\n",
        "\n",
        "        x = self._out(x, training=for_training)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "UQumN6U21BLh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "Bq-MZlpZ13Pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(g, d, imgs, loss_function, g_optim, d_optim, for_training):\n",
        "    # generates a random image\n",
        "    noise = tf.random.normal([64,100])\n",
        "\n",
        "    with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
        "\n",
        "        # generate noise vector\n",
        "        generated_imgs = g(noise, training=for_training)\n",
        "\n",
        "        real_out = d(imgs, training=for_training)\n",
        "        fake_out = d(generated_imgs, training=for_training)\n",
        "\n",
        "        # calculating losses\n",
        "        g_loss = loss_function(tf.ones_like(fake_out), fake_out)\n",
        "        d_loss = loss_function(tf.ones_like(real_out)*0.9, real_out) + loss_function(tf.zeros_like(fake_out), fake_out)\n",
        "\n",
        "        # calculaing the gradients\n",
        "        gradients_g = g_tape.gradient(g_loss, g.trainable_variables)\n",
        "        gradients_d = d_tape.gradient(d_loss, d.trainable_variables)\n",
        "\n",
        "    # updating weights and biases\n",
        "    g_optim.apply_gradients(zip(gradients_g, g.trainable_variables))\n",
        "    d_optim.apply_gradients(zip(gradients_d, d.trainable_variables))\n",
        "\n",
        "    return g_loss, d_loss\n",
        "\n",
        "\n",
        "def test(gen, disc, test_data, loss_function, for_training):\n",
        "    # initializing lists for accuracys and loss\n",
        "    accuracy_aggregator = []\n",
        "    g_loss_aggregator = []\n",
        "    d_loss_aggregator = []\n",
        "\n",
        "    for imgs in test_data:\n",
        "        # forward step\n",
        "        noise = tf.random.normal([64,100])\n",
        "        generated_imgs = g(noise, training=for_training)\n",
        "        real_out = d(imgs, training=for_training)\n",
        "        fake_out = d(generated_imgs, training=for_training)\n",
        "\n",
        "        # calculating loss\n",
        "        g_loss = loss_function(tf.ones_like(fake_out), fake_out)\n",
        "        d_loss = loss_function(tf.ones_like(real_out)*0.9, real_out) + loss_function(tf.zeros_like(fake_out)*0.95, fake_out)\n",
        "\n",
        "        # add loss and accuracy to the lists\n",
        "        g_loss_aggregator.append(g_loss.numpy())\n",
        "        d_loss_aggregator.append(d_loss.numpy())\n",
        "\n",
        "    # calculate the mean of the loss and accuracy (for this epoch)\n",
        "    g_loss = tf.reduce_mean(g_loss_aggregator)\n",
        "    d_loss = tf.reduce_mean(d_loss_aggregator)\n",
        "\n",
        "    return g_loss, d_loss\n"
      ],
      "metadata": {
        "id": "3RYnG5fW15wJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classification(g, d, num_epochs, train_ds, valid_ds):\n",
        "    seed = tf.random.normal([64,100])\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    # initialize the loss: categorical cross entropy\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    lr = 0.0002\n",
        "\n",
        "    # optimizers for both models\n",
        "    g_optim = tf.keras.optimizers.Adam(lr,beta_1=0.5)\n",
        "    d_optim = tf.keras.optimizers.Adam(lr,beta_1=0.5)\n",
        "\n",
        "    # initialize lists for later visualization.\n",
        "    train_g_losses = []\n",
        "    valid_g_losses = []\n",
        "    train_d_losses = []\n",
        "    valid_d_losses = []\n",
        "\n",
        "    # testing on our valid_ds once before we begin\n",
        "    valid_g_loss, valid_d_loss = test(g, d, valid_ds, loss, for_training=False)\n",
        "    valid_g_losses.append(valid_g_loss)\n",
        "    valid_d_losses.append(valid_d_loss)\n",
        "\n",
        "    # Testing on our train_ds once before we begin\n",
        "    train_g_loss, train_d_loss = test(g, d, train_ds, loss, for_training=False)\n",
        "    train_g_losses.append(train_g_loss)\n",
        "    train_d_losses.append(train_d_loss)\n",
        "\n",
        "    # training our model for num_epochs epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f' starting with (validation set): g_loss {valid_g_losses[-1]} and d_loss {valid_d_losses[-1]}')\n",
        "        print(f' and (training set): g_loss {train_g_losses[-1]} and d_loss {train_d_losses[-1]}')\n",
        "        print(\"{}/{} epoches\".format(epoch, num_epochs))\n",
        "        \n",
        "        # training (and calculating loss while training)\n",
        "        epoch_g_loss_agg = []\n",
        "        epoch_d_loss_agg = []\n",
        "\n",
        "        for imgs in train_ds:\n",
        "            train_g_loss, train_d_loss = train_step(g, d, imgs, loss, g_optim, d_optim, for_training=True)\n",
        "            epoch_g_loss_agg.append(train_g_loss)\n",
        "            epoch_d_loss_agg.append(train_d_loss)\n",
        "\n",
        "        # track training loss\n",
        "        train_g_losses.append(tf.reduce_mean(epoch_g_loss_agg))\n",
        "        train_d_losses.append(tf.reduce_mean(epoch_d_loss_agg))\n",
        "\n",
        "        ## After i-th epoch plot image\n",
        "        if (epoch % 5) == 0:\n",
        "            fake_image = tf.reshape(g(seed, for_training=False), shape = (64,28,28))\n",
        "            plt.imshow(fake_image[10], cmap = \"gray\")\n",
        "            plt.show()\n",
        "\n",
        "        # testing our model in each epoch to track accuracy and loss on the validation set\n",
        "        valid_g_loss, valid_d_loss = test(g, d, valid_ds, loss, for_training=False)\n",
        "        valid_g_losses.append(valid_g_loss)\n",
        "        valid_d_losses.append(valid_d_loss)\n",
        "\n",
        "    results = [train_g_losses, valid_g_losses, train_d_losses, valid_d_losses]\n",
        "    return results, g, d"
      ],
      "metadata": {
        "id": "9yhtsKXs3HjX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_results(train_g_losses, valid_g_losses, train_d_losses, valid_d_losses):\n",
        "\n",
        "    fig, axs = plt.subplots(2, 1)\n",
        "    #fig.set_size_inches(13, 6)\n",
        "    # making a grid with subplots\n",
        "    for j in range(1):\n",
        "        axs[0].plot(train_g_l[j])\n",
        "        axs[0].plot(valid_g_l[j])\n",
        "        axs[1].plot(train_d_l[j])\n",
        "        axs[1].plot(valid_d_l[j])\n",
        "        axs[1].sharex(axs[0])\n",
        "\n",
        "    fig.legend([\" train_g_l\",\" valid_g_l\",\" train_d_l\",\" valid_d_l\"],loc=\"lower right\")\n",
        "    plt.xlabel(\"Training epoch\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "CyQUQOb33P6W"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "train_ds, valid_ds, test_ds = load_data()\n",
        "\n",
        "d = Descriminator()\n",
        "g = Generator()\n",
        "\n",
        "train_g_losses = []\n",
        "valid_g_losses = []\n",
        "train_d_losses = []\n",
        "valid_d_losses = []\n",
        "\n",
        "\n",
        "with tf.device('/device:gpu:0'):\n",
        "    # training the model\n",
        "    results, trained_g, trained_d = classification(g, d, 10, train_ds, valid_ds)\n",
        "\n",
        "    # saving results for visualization\n",
        "    train_g_losses.append(results[0])\n",
        "    valid_g_losses.append(results[1])\n",
        "    train_d_losses.append(results[2])\n",
        "    valid_d_losses.append(results[3])\n",
        "\n",
        "    # testing the trained model\n",
        "    _, test_accuracy = test(trained_model, test_ds, tf.keras.losses.BinaryCrossentropy(), False)\n",
        "    print(\"Accuracy (test set):\", test_accuracy)\n",
        "\n",
        "# visualizing losses and accuracy\n",
        "visualize_results(train_g_losses, valid_g_losses, train_d_losses, valid_d_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N-5mqwwtPnpJ",
        "outputId": "9e6ccdb0-202a-4dbd-c311-97c603e7841d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "141545 images to train on\n",
            " starting with (validation set): g_loss 0.6924867033958435 and d_loss 1.3940385580062866\n",
            " and (training set): g_loss 0.6924607157707214 and d_loss 1.3942387104034424\n",
            "0/10 epoches\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " starting with (validation set): g_loss 1.070691466331482 and d_loss 1.086503505706787\n",
            " and (training set): g_loss 0.9579606652259827 and d_loss 1.196457028388977\n",
            "1/10 epoches\n",
            " starting with (validation set): g_loss 1.1470497846603394 and d_loss 1.0514775514602661\n",
            " and (training set): g_loss 1.0479917526245117 and d_loss 1.1837100982666016\n",
            "2/10 epoches\n",
            " starting with (validation set): g_loss 1.1831750869750977 and d_loss 1.0685020685195923\n",
            " and (training set): g_loss 1.1106207370758057 and d_loss 1.145597219467163\n",
            "3/10 epoches\n",
            " starting with (validation set): g_loss 1.0800505876541138 and d_loss 1.0175713300704956\n",
            " and (training set): g_loss 1.1498342752456665 and d_loss 1.1220132112503052\n",
            "4/10 epoches\n",
            " starting with (validation set): g_loss 1.2409369945526123 and d_loss 1.0175076723098755\n",
            " and (training set): g_loss 1.1949028968811035 and d_loss 1.110071063041687\n",
            "5/10 epoches\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " starting with (validation set): g_loss 1.4881033897399902 and d_loss 1.0136141777038574\n",
            " and (training set): g_loss 1.214905023574829 and d_loss 1.1020742654800415\n",
            "6/10 epoches\n",
            " starting with (validation set): g_loss 1.0028889179229736 and d_loss 1.0073235034942627\n",
            " and (training set): g_loss 1.236133098602295 and d_loss 1.0973445177078247\n",
            "7/10 epoches\n",
            " starting with (validation set): g_loss 1.1005479097366333 and d_loss 1.000794768333435\n",
            " and (training set): g_loss 1.2520748376846313 and d_loss 1.0910727977752686\n",
            "8/10 epoches\n",
            " starting with (validation set): g_loss 1.3315298557281494 and d_loss 1.0031533241271973\n",
            " and (training set): g_loss 1.252660870552063 and d_loss 1.0952441692352295\n",
            "9/10 epoches\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-5d1e0df5a4b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# testing the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBinaryCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy (test set):\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trained_model' is not defined"
          ]
        }
      ]
    }
  ]
}