{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework_3_iannwtf.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBsaVuLDpemC"
      },
      "source": [
        "Libraries needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZRDLWC3pboK"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70fbVBOrpmlA"
      },
      "source": [
        "# import dataset\n",
        "(training_genoms, training_labels), (test_genoms, test_labels) = tfds.load(\n",
        "    \"genomics_ood\",\n",
        "    as_supervised=True, # to include the labels\n",
        "    split=[\"train[0:100000]\", \"test[0:1000]\"], #100.000 example for training and 1000 for testing\n",
        "    batch_size=(-1) # -1 to get the full dataset as a tf.Tensor\n",
        "    )\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5ltd24flvMw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYqGwf3Cqlx6"
      },
      "source": [
        "def onehotify(tensor):\n",
        "  vocab = {'A':'1', 'C': '2', 'G':'3', 'T':'0'}\n",
        "  for key in vocab.keys():\n",
        "    tensor = tf.strings.regex_replace(tensor, key, vocab[key])\n",
        "    split = tf.strings.bytes_split(tensor)\n",
        "    labels = tf.cast(tf.strings.to_number(split), tf.uint8)\n",
        "    onehot = tf.one_hot(labels, 4)\n",
        "    onehot = tf.reshape(onehot, (-1,))\n",
        "  return onehot\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBjqcWWZnF_u"
      },
      "source": [
        "need to zip the data and lebels and run onehotify\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W7TEBWlnHSu"
      },
      "source": [
        "# preprocessing the data\n",
        "\n",
        "# tensorflow.python.framework.ops.EagerTensor has to attribute map\n",
        "# so converting the labels into datasets using tf.data.Dataset.from_tensor_slices\n",
        "\n",
        "# https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/data.ipynb\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices(training_genoms)\n",
        "train_label = tf.data.Dataset.from_tensor_slices(training_labels)\n",
        "\n",
        "\n",
        "genoms_ds = train_data.map(onehotify)\n",
        "#labels_ds =  ??\n",
        "\n",
        "# put it together\n",
        "train_ds = tf.data.Dataset.zip((genoms_ds, labels_ds))\n",
        "\n",
        "\n",
        "# batch it\n",
        "\n",
        "train_ds.batch()\n",
        "\n",
        "\n",
        "# shuffel it\n",
        "train_ds.shuffle()\n",
        "\n",
        "# prefetch it\n",
        "train_ds.prefetch()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# do the same things for training data set\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x549UBNisY76"
      },
      "source": [
        "Designing the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oU4xltvLsa9J"
      },
      "source": [
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense , Activation\n",
        "\n",
        "\n",
        "# model = Sequential()\n",
        "\n",
        "# model.add(tf.keras.layers.Dense(256, activation=tf.nn.sigmoid))\n",
        "# model.add(tf.keras.layers.Dense(256, activation=tf.nn.softmax))\n",
        "# model.add(tf.keras.layers.Dense(10, activation=tf.nn.relu))\n",
        "\n",
        "# model.compile(optimizer='sgd', loss=tf.keras.losses.CategoricalCrossentropy())\n",
        "\n",
        "# #model.fit(training_genoms,training_labels, epochs=500)\n",
        "# #model.summary(line_length=None, positions=None, print_fn=None)\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(256, activation=tf.nn.sigmoid)\n",
        "        self.dense2 = tf.keras.layers.Dense(256, activation=tf.nn.sigmoid)\n",
        "        self.output = tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "\n",
        "        x = self.dense2(x)\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oT9rJGsAkLeW"
      },
      "source": [
        "Training, mainly taken from courseware\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uC31gEjj6qn"
      },
      "source": [
        "def train_step(model, input, target, loss_function, optimizer):\n",
        "  # loss_object and optimizer_object are instances of respective tensorflow classes\n",
        "  with tf.GradientTape() as tape:\n",
        "    prediction = model(input)\n",
        "    loss = loss_function(target, prediction)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  return loss\n",
        "\n",
        "def test(model, test_data, loss_function):\n",
        "  # test over complete test data\n",
        "\n",
        "  test_accuracy_aggregator = []\n",
        "  test_loss_aggregator = []\n",
        "\n",
        "  for (input, target) in test_data:\n",
        "    prediction = model(input)\n",
        "    sample_test_loss = loss_function(target, prediction)\n",
        "    sample_test_accuracy =  np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
        "    sample_test_accuracy = np.mean(sample_test_accuracy)\n",
        "    test_loss_aggregator.append(sample_test_loss.numpy())\n",
        "    test_accuracy_aggregator.append(np.mean(sample_test_accuracy))\n",
        "\n",
        "  test_loss = tf.reduce_mean(test_loss_aggregator)\n",
        "  test_accuracy = tf.reduce_mean(test_accuracy_aggregator)\n",
        "\n",
        "  return test_loss, test_accuracy"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-gpkfDJkKXq"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "OcOyqWXbkJaz",
        "outputId": "412618ce-835c-4361-d7fb-43e90b92fed2"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#For showcasing we only use a subset of the training and test data (generally use all of the available data!)\n",
        "train_dataset = train_dataset.take(1000)\n",
        "test_dataset = test_dataset.take(100)\n",
        "\n",
        "### Hyperparameters\n",
        "num_epochs = 10\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Initialize the model.\n",
        "model = MyModel()\n",
        "# Initialize the loss: categorical cross entropy. Check out 'tf.keras.losses'.\n",
        "cross_entropy_loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "# Initialize the optimizer: SGD with default parameters. Check out 'tf.keras.optimizers'\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "\n",
        "# Initialize lists for later visualization.\n",
        "train_losses = []\n",
        "\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "#testing once before we begin\n",
        "test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
        "test_losses.append(test_loss)\n",
        "test_accuracies.append(test_accuracy)\n",
        "\n",
        "#check how model performs on train data once before we begin\n",
        "train_loss, _ = test(model, train_dataset, cross_entropy_loss)\n",
        "train_losses.append(train_loss)\n",
        "\n",
        "# We train for num_epochs epochs.\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch: {str(epoch)} starting with accuracy {test_accuracies[-1]}')\n",
        "\n",
        "    #training (and checking in with training)\n",
        "    epoch_loss_agg = []\n",
        "    for input,target in train_dataset:\n",
        "        train_loss = train_step(model, input, target, cross_entropy_loss, optimizer)\n",
        "        epoch_loss_agg.append(train_loss)\n",
        "    \n",
        "    #track training loss\n",
        "    train_losses.append(tf.reduce_mean(epoch_loss_agg))\n",
        "\n",
        "    #testing, so we can track accuracy and test loss\n",
        "    test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-b75c0164abaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#For showcasing we only use a subset of the training and test data (generally use all of the available data!)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'take'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-3NagyX9Dn6"
      },
      "source": [
        "visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS451yMA9FLN"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize accuracy and loss for training and test data.\n",
        "plt.figure()\n",
        "line1, = plt.plot(train_losses)\n",
        "line2, = plt.plot(test_losses)\n",
        "line3, = plt.plot(test_accuracies)\n",
        "plt.xlabel(\"Training steps\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend((line1,line2, line3),(\"training\",\"test\", \"test accuracy\"))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}