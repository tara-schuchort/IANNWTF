{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRigAa6RBiak"
   },
   "source": [
    "### 1 Data set\n",
    "\n",
    "\n",
    "In this homework we want to classify bacteria based on their genome sequences. The 'genomics_ood' data set is already inbuilt in Tensorflow. The input will be a genomic sequence, which (short biology recap) consists of the characters $\\{\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}\\}$ (also known as nucleotides). Each genomic sequence will be 250 characters long, with each char one of the 4 options. We want to classify each genome sequence to 1 of 10 bacteria populations. (Technically, the data set also contains 120 more 'out of distribution' classes which are stored in test_ood and validation_ood, but you can disregard them for this homework.) Learn more about the data set here and have a look at example elements: https: //wWW.tensorflow.org/datasets/catalog/genomics_ood\n",
    "\n",
    "First you will need to load the data set from the module tensorflow_datasets. 1\n",
    "\n",
    "We have heard of some troubles with this data set and Windows. In case you encounter error messages when loading the data set when running it on your own machine, please consider switching to Colab. (Or trying a dual boot Ubuntu option. If none of that is an option for you, we recommend you working together with group mates who can load the data set.)\n",
    "\n",
    "Then we will need some preparations on the data:\n",
    "\n",
    "Take out a bunch of examples for our network. The whole dataset contains 1 million training examples and $100.000$ test examples. It will be sufficient to use $100.000$ examples for training and 1000 for testing. (If you are not sure what is the purpose of having a training and testing data set, check the chapter 'Data and Data Pipelines' on Courseware Week 03 again. But we will also talk about that next week in more detail.)\n",
    "\n",
    "The genomic sequences come as string-tensor which are not so handy to work with. Instead, we would like to have the genomic sequence with one-hot-vectors for encoding the nucleotides $(\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T})$. But because the sequences are string tensors, we cannot simply call tf. one_hot $(x, 4)$ on them. Therefore we need a function which converts the string tensor into a usable tensor that contains the one-hot-encoded sequence. You can try to build this function yourself (which might potentially be tricky, but we will give you some instructions in the hints $^{2}$ ). Otherwise we also provide it for you in the end of the document. Also apply one-hot-encoding on the labels. (Keep in mind that they have a different shape than the input data.) $^{3}$\n",
    "\n",
    "For an outstanding we would like to see you using an input pipeline, including batching and (ideally) prefetching, but otherwise you can also do it without one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wT3Rm0BLBytL"
   },
   "source": [
    "${ }^{1}$ Check out tfds. load () for that. Make sure you have as_supervised = True, because otherwise the labels are not included in the data set.\n",
    "\n",
    "${ }^{2}$ First replace the character with a number from $0-3 .$ You might have to translate the \"string\" numbers into int numbers - check out tf.cast() for that. Split after each number. And only then call tf. one_hot $(x, 4)$ on them. It is also important that your inputs have the shape (batchsize, 1000$)$ in the end. Or simply check the function provided in the end of the document.\n",
    "\n",
    "${ }^{3}$ You can use the standard tf. one_hot () for the labels. Your labels should have the shape (batchsize, 10) in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZRDLWC3pboK"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bqads4NkBhkz"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70fbVBOrpmlA"
   },
   "outputs": [],
   "source": [
    "# import dataset\n",
    "(training_genoms, training_labels), (test_genoms, test_labels) = tfds.load(\n",
    "    \"genomics_ood\",\n",
    "    as_supervised=True, # to include the labels\n",
    "    split=[\"train[0:100000]\", \"test[0:1000]\"], #100.000 example for training and 1000 for testing\n",
    "    batch_size=(-1) # -1 to get the full dataset as a tf.Tensor\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5ltd24flvMw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYqGwf3Cqlx6"
   },
   "outputs": [],
   "source": [
    "def onehotify(tensor):\n",
    "  vocab = {'A':'1', 'C': '2', 'G':'3', 'T':'0'}\n",
    "  for key in vocab.keys():\n",
    "    tensor = tf.strings.regex_replace(tensor, key, vocab[key])\n",
    "    split = tf.strings.bytes_split(tensor)\n",
    "    labels = tf.cast(tf.strings.to_number(split), tf.uint8)\n",
    "    onehot = tf.one_hot(labels, 4)\n",
    "    onehot = tf.reshape(onehot, (-1,))\n",
    "  return onehot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBjqcWWZnF_u"
   },
   "source": [
    "need to zip the data and lebels and run onehotify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0W7TEBWlnHSu"
   },
   "outputs": [],
   "source": [
    "# preprocessing the data\n",
    "\n",
    "# tensorflow.python.framework.ops.EagerTensor has to attribute map\n",
    "# so converting the labels into datasets using tf.data.Dataset.from_tensor_slices\n",
    "\n",
    "# https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/data.ipynb\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices(training_genoms)\n",
    "train_label = tf.data.Dataset.from_tensor_slices(training_labels)\n",
    "\n",
    "\n",
    "genoms_ds = train_data.map(onehotify)\n",
    "#labels_ds =  ??\n",
    "\n",
    "# put it together\n",
    "train_ds = tf.data.Dataset.zip((genoms_ds, labels_ds))\n",
    "\n",
    "\n",
    "# batch it\n",
    "\n",
    "train_ds.batch()\n",
    "\n",
    "\n",
    "# shuffel it\n",
    "train_ds.shuffle()\n",
    "\n",
    "# prefetch it\n",
    "train_ds.prefetch()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# do the same things for training data set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x549UBNisY76"
   },
   "source": [
    "### Model\n",
    "\n",
    "We will implement a simple fully connected feed forward neural network like the last time. Our network will have the following layers:\n",
    "\n",
    "- Hidden layer 1: 256 units. With sigmoid activation function.\n",
    "\n",
    "- Hidden layer 2: 256 units. With sigmoid activation function.\n",
    "\n",
    "- Output: 10 units. With softmax activation function.\n",
    "\n",
    "You can start by implementing your own dense layer class and then use it for your custom model $^{4}$\n",
    "\n",
    "\n",
    "\n",
    "${ }^{4}$ Check out the chapter: \"Models and Layers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oU4xltvLsa9J"
   },
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense , Activation\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(tf.keras.layers.Dense(256, activation=tf.nn.sigmoid))\n",
    "# model.add(tf.keras.layers.Dense(256, activation=tf.nn.softmax))\n",
    "# model.add(tf.keras.layers.Dense(10, activation=tf.nn.relu))\n",
    "\n",
    "# model.compile(optimizer='sgd', loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "\n",
    "# #model.fit(training_genoms,training_labels, epochs=500)\n",
    "# #model.summary(line_length=None, positions=None, print_fn=None)\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(256, activation=tf.nn.sigmoid)\n",
    "        self.dense2 = tf.keras.layers.Dense(256, activation=tf.nn.sigmoid)\n",
    "        self.output = tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "\n",
    "        x = self.dense2(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQczlY04CHXG"
   },
   "source": [
    "### Training\n",
    "\n",
    "Then train your network for 10 epochs using a learning rate of $0.1$. As a loss use the categorical cross entropy. ${ }^{5}$ As an optimizer use SGD.\n",
    "\n",
    "For the training loop you can use the \"Complete_Model_Training\" notebook on Courseware as orientation or try to build one yourself.\n",
    "\n",
    "For this task, an accuracy of $35-40 \\%$ is sufficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWQ_wVujCKPZ"
   },
   "source": [
    "${ }^{5}$ Check out 'tf.keras.losses'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oT9rJGsAkLeW"
   },
   "source": [
    "Training, mainly taken from courseware\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2uC31gEjj6qn"
   },
   "outputs": [],
   "source": [
    "def train_step(model, input, target, loss_function, optimizer):\n",
    "  # loss_object and optimizer_object are instances of respective tensorflow classes\n",
    "  with tf.GradientTape() as tape:\n",
    "    prediction = model(input)\n",
    "    loss = loss_function(target, prediction)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  return loss\n",
    "\n",
    "def test(model, test_data, loss_function):\n",
    "  # test over complete test data\n",
    "\n",
    "  test_accuracy_aggregator = []\n",
    "  test_loss_aggregator = []\n",
    "\n",
    "  for (input, target) in test_data:\n",
    "    prediction = model(input)\n",
    "    sample_test_loss = loss_function(target, prediction)\n",
    "    sample_test_accuracy =  np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
    "    sample_test_accuracy = np.mean(sample_test_accuracy)\n",
    "    test_loss_aggregator.append(sample_test_loss.numpy())\n",
    "    test_accuracy_aggregator.append(np.mean(sample_test_accuracy))\n",
    "\n",
    "  test_loss = tf.reduce_mean(test_loss_aggregator)\n",
    "  test_accuracy = tf.reduce_mean(test_accuracy_aggregator)\n",
    "\n",
    "  return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-gpkfDJkKXq"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "OcOyqWXbkJaz",
    "outputId": "412618ce-835c-4361-d7fb-43e90b92fed2"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b75c0164abaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#For showcasing we only use a subset of the training and test data (generally use all of the available data!)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'take'"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "#For showcasing we only use a subset of the training and test data (generally use all of the available data!)\n",
    "train_dataset = train_dataset.take(1000)\n",
    "test_dataset = test_dataset.take(100)\n",
    "\n",
    "### Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize the model.\n",
    "model = MyModel()\n",
    "# Initialize the loss: categorical cross entropy. Check out 'tf.keras.losses'.\n",
    "cross_entropy_loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "# Initialize the optimizer: SGD with default parameters. Check out 'tf.keras.optimizers'\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "\n",
    "# Initialize lists for later visualization.\n",
    "train_losses = []\n",
    "\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "#testing once before we begin\n",
    "test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
    "test_losses.append(test_loss)\n",
    "test_accuracies.append(test_accuracy)\n",
    "\n",
    "#check how model performs on train data once before we begin\n",
    "train_loss, _ = test(model, train_dataset, cross_entropy_loss)\n",
    "train_losses.append(train_loss)\n",
    "\n",
    "# We train for num_epochs epochs.\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch: {str(epoch)} starting with accuracy {test_accuracies[-1]}')\n",
    "\n",
    "    #training (and checking in with training)\n",
    "    epoch_loss_agg = []\n",
    "    for input,target in train_dataset:\n",
    "        train_loss = train_step(model, input, target, cross_entropy_loss, optimizer)\n",
    "        epoch_loss_agg.append(train_loss)\n",
    "    \n",
    "    #track training loss\n",
    "    train_losses.append(tf.reduce_mean(epoch_loss_agg))\n",
    "\n",
    "    #testing, so we can track accuracy and test loss\n",
    "    test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-3NagyX9Dn6"
   },
   "source": [
    "### visualization\n",
    "\n",
    "\n",
    "\n",
    "Visualize accuracy and loss for training and test data using matplotlib. ${ }^{6}$ \n",
    "\n",
    "${ }^{6}$ Use the notebooks uploaded by us as orientation, e.g. the \"Complete_Model_Training\" notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sS451yMA9FLN"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize accuracy and loss for training and test data.\n",
    "plt.figure()\n",
    "line1, = plt.plot(train_losses)\n",
    "line2, = plt.plot(test_losses)\n",
    "line3, = plt.plot(test_accuracies)\n",
    "plt.xlabel(\"Training steps\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend((line1,line2, line3),(\"training\",\"test\", \"test accuracy\"))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "homework_3_iannwtf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
