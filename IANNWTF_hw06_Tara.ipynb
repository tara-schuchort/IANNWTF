{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IANNWTF_hw06.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJgGMUpJ36d+rG7M+mIXAP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tara-schuchort/IANNWTF/blob/main/IANNWTF_hw06_Tara.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E4GE-ROZSq9"
      },
      "source": [
        "All necessary imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTp3x5glZFoY"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, Conv2D, GlobalAveragePooling2D, Dense, MaxPool2D, AveragePooling2D, Flatten, Add, Concatenate\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2Il-NJlZWoC"
      },
      "source": [
        "# loading the dataset\n",
        "train_download, test_download = tfds.load(\"cifar10\", split=[\"train\", \"test\"], as_supervised=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndg5tKFYZW5I"
      },
      "source": [
        "Piping the dataset through the pipeline to prepare it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQPopdtiZXBL"
      },
      "source": [
        "def data_pipeline(data):\n",
        "  \"\"\" Describtion here\n",
        "  Args:\n",
        "    data:\n",
        "  Return:\n",
        "    data:\n",
        "  \"\"\"\n",
        "\n",
        "  data = data.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
        "\n",
        "  #normalization, brings image values from range [0, 255] to [-1, 1]\n",
        "  data = data.map(lambda img, target: ((img/128.)-1., target))\n",
        "  #data = data.map(lambda img, target: ((img/255.), target)) alternative?\n",
        "\n",
        "  #create one-hot targets\n",
        "  data = data.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
        "\n",
        "  #cache progress in memory, as there is no need to redo it\n",
        "  data = data.cache()\n",
        "\n",
        "  #shuffle, batch, prefetch\n",
        "  data = data.shuffle(1000)\n",
        "  data = data.batch(64)\n",
        "  data = data.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "  #return preprocessed dataset\n",
        "  return data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lubLSbxcZXHn"
      },
      "source": [
        "# Models\n",
        "\n",
        "ResBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P9TrIcDdfqm"
      },
      "source": [
        "class ResidualBlock(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_filters):\n",
        "\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.batchnorm1 = BatchNormalization()\n",
        "        self.activation1 = Activation(\"relu\")\n",
        "        self.convolution1 = Conv2D(filters=num_filters, kernel_size=(1,1))\n",
        "\n",
        "        self.batchnorm2 = BatchNormalization()\n",
        "        self.activation2 = Activation(\"relu\")\n",
        "        self.convolution1 = Conv2D(filters=num_filters, kernel_size=(3,3), padding=\"same\")\n",
        "\n",
        "        self.batchnorm3 = BatchNormalization()\n",
        "        self.activation3 = Activation(\"relu\")\n",
        "        self.convolution3 = Conv2D(filters=num_filters, kernel_size=(1,1))\n",
        "\n",
        "        self.add = Add()\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs, for_training=None):\n",
        "\n",
        "        x = self.batchnorm1(inputs, training=for_training)\n",
        "        x = self.activation1(x)\n",
        "        x = self.convolution1(x)\n",
        "\n",
        "        x = self.batchnorm2(x, training=for_training)\n",
        "        x = self.activation2(x)\n",
        "        x = self.convolution2(x)\n",
        "\n",
        "        x = self.batchnorm3(x, training=for_training)\n",
        "        x = self.activation3(x)\n",
        "        x = self.convolution3(x)\n",
        "\n",
        "        x = self.add([x, inputs])\n",
        "\n",
        "        return x\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGwjJq7AlxBk"
      },
      "source": [
        "ResNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQTwSl_RlwvR"
      },
      "source": [
        "class ResNet(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_blocks, num_block_filters):\n",
        "\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.convolutional = Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\", padding=\"same\",  input_shape=(32, 32, 3))\n",
        "        # self.bn = BatchNormalization()\n",
        "        # self.pool = MaxPool2D(pool_size = 3,strides = 2)\n",
        "\n",
        "        # residual blocks\n",
        "        self.blocks = [ResidualBlock(num_filters=num_block_filters) for index in range(num_blocks)]\n",
        "\n",
        "        # classification\n",
        "        self.global_pool = GlobalAvgPool2D()\n",
        "        self.out = Dense(10, kernel_regularizer=\"l1_l2\", activation=\"softmax\")\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs, for_training):\n",
        "        \n",
        "        x = self.convolutional(inputs)\n",
        "        #x = self.bn(x,training=is_training)\n",
        "        #x = tf.nn.relu(x)\n",
        "        #x = self.pool(x)\n",
        "\n",
        "        # residual blocks\n",
        "        for resblock in self.blocks:\n",
        "            x = block(x, training=for_training)\n",
        "\n",
        "        # classification\n",
        "        x = self.global_pool(x)\n",
        "        x = self.out(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeNrMQhImTzK"
      },
      "source": [
        "Transition Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AP4zzW5mVXN"
      },
      "source": [
        "class TransitionLayer(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_filters):\n",
        "\n",
        "        super(TransitionLayer, self).__init__()\n",
        "\n",
        "        self.convolutional = Conv2D(filters=num_filters, kernel_size=(1,1), padding=\"same\")  # or maybe padding = valid?\n",
        "        self.batchnorm = BatchNormalization()\n",
        "        self.activation = Activation(\"relu\")\n",
        "        self.pooling = AveragePooling2D(pool_size = 2, strides=(2,2), padding=\"same\") # maybe leave this out\n",
        "\n",
        "    \n",
        "    @tf.function\n",
        "    def call(self, inputs, for_training=None):\n",
        "\n",
        "        x = self.convolutional(inputs)\n",
        "        x = self.batchnorm(x, training=for_training)\n",
        "        x = self.activation(x)\n",
        "        x = self.pooling(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVphriLEmBTX"
      },
      "source": [
        "DenseBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZvgvaJBmFP1"
      },
      "source": [
        "class DenseBlock(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_filters, new_channels):\n",
        "\n",
        "        super(DenseBlock, self).__init__()\n",
        "\n",
        "        self.batchnorm1 = BatchNormalization()\n",
        "        self.activation1 = Activation(\"relu\")\n",
        "        self.convolution1 = Conv2D(filters=num_filters, kernel_size=(1,1), padding=\"valid\")\n",
        "\n",
        "        self.batchnorm2 = BatchNormalization()\n",
        "        self.activation2 = Activation(\"relu\")\n",
        "        self.conv2 = Conv2D(filters=new_channels, kernel_size=(3,3), padding=\"same\")\n",
        "\n",
        "        self.concat = Concatenate(axis=-1)\n",
        "\n",
        "    \n",
        "    @tf.function\n",
        "    def call(self, inputs, for_training=None):\n",
        "\n",
        "        x = self.batchnorm1(inputs, training=for_training)\n",
        "        x = self.activation1(x)\n",
        "        x = self.convolution1(x)\n",
        "\n",
        "        x = self.batchnorm2(x, training=for_training)\n",
        "        x = self.activation2(x)\n",
        "        x = self.convolution2(x)\n",
        "\n",
        "        x = self.concat([x, inputs])\n",
        "\n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Aw4_74mmFgo"
      },
      "source": [
        "DenseNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODSSEg8DmTXv"
      },
      "source": [
        "class DenseNet(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_blocks, num_filters, new_channels, growth_rate):\n",
        "\n",
        "        super(DenseNet, self).__init__()\n",
        "        \n",
        "        self.convolutional = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\", input_shape=(32, 32, 3))\n",
        "        self.batchnorm = BatchNormalization()\n",
        "\n",
        "        self.blocks = []\n",
        "        for index in range(num_blocks):\n",
        "            self.blocks.append(DenseBlock(num_filters=num_filters, new_channels=new_channels))\n",
        "            self.blocks.append(TransitionLayer(num_filters=growth_rate*2))\n",
        "        self.blocks.append(DenseBlock(num_filters=num_filters, new_channels=new_channels))\n",
        "\n",
        "        self.global_pool = GlobalAveragePooling2D()\n",
        "        self.out = Dense(10, activation=\"softmax\")\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs, for_training=None):\n",
        "\n",
        "        x = self.convolutional(inputs)\n",
        "        x = self.batchnorm(x, training=for_training)\n",
        "        \n",
        "        for index in range(len(self.blocks)):\n",
        "            x = self.blocks[index](x, training=for_training)\n",
        "        \n",
        "        x = self.global_pool(x)\n",
        "        x = self.out(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSYjv5VimVhf"
      },
      "source": [
        "# Training and Testing\n",
        "Implementation of test and training functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfGDTA_enN1B"
      },
      "source": [
        "@tf.function\n",
        "def train_step(model, inputs, target, loss_function, optimizer):\n",
        "    # loss_object and optimizer_object are instances of respective tensorflow classes\n",
        "    with tf.GradientTape() as tape:\n",
        "        prediction = model(inputs, for_training=True)\n",
        "        # model.losses then need to reduce to single value\n",
        "        loss = loss_function(target, prediction)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    \n",
        "    return loss\n",
        "\n",
        "\n",
        "def test(model, test_data, loss_function):\n",
        "    # test over complete test data\n",
        "    test_accuracy_aggregator = np.empty(0)\n",
        "    test_loss_aggregator = np.empty(0)\n",
        "\n",
        "    for (input, target) in test_data:\n",
        "        prediction = model(input, for_training=False)\n",
        "        #print(prediction.shape)\n",
        "        sample_test_loss = loss_function(target, prediction)\n",
        "        sample_test_accuracy =  np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
        "        sample_test_accuracy = np.mean(sample_test_accuracy)\n",
        "        test_loss_aggregator = np.append(test_loss_aggregator, sample_test_loss)\n",
        "        test_accuracy_aggregator = np.append(test_accuracy_aggregator, sample_test_accuracy)\n",
        "    \n",
        "    test_loss = tf.reduce_mean(test_loss_aggregator)\n",
        "    test_accuracy = tf.reduce_mean(test_accuracy_aggregator)\n",
        "\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4iSG6NCnN-s"
      },
      "source": [
        "Execution of training and testing on the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxCil7conRG4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        },
        "outputId": "8c0678e3-c97f-4f69-9fb9-ab4c634f7ab6"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Prepare the data\n",
        "test_dataset = data_pipeline(test_download)\n",
        "train_dataset = data_pipeline(train_download)\n",
        "\n",
        "### Hyperparameters\n",
        "num_epochs = 30\n",
        "learning_rate = 0.001  # tf.constant(0.001, dtype=tf.float32)\n",
        "\n",
        "# Initialize the model\n",
        "model = DenseNet(num_blocks=2, num_filters=128, new_channels=32, growth_rate=32)\n",
        "\n",
        "# Initialize the loss function.\n",
        "global_loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "# Initialize the optimizer:\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# Initialize numpy arrays for later visualization\n",
        "train_losses = np.empty(0)\n",
        "test_losses = np.empty(0)\n",
        "test_accuracies = np.empty(0)\n",
        "train_accuracies = np.empty(0)\n",
        "\n",
        "# testing once before we begin\n",
        "test_loss, test_accuracy = test(model, test_dataset, global_loss_function)\n",
        "test_losses = np.append(test_losses, test_loss)\n",
        "test_accuracies = np.append(test_accuracies, test_accuracy)\n",
        "\n",
        "# check how model performs on train data once before we begin\n",
        "train_loss, _ = test(model, train_dataset, global_loss_function)\n",
        "train_losses = np.append(train_losses, train_loss)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# We train for num_epochs epochs.\n",
        "for epoch in range(num_epochs):\n",
        "    # Display accuracy at the beginning of each epoch\n",
        "    print(f'Epoch: {str(epoch)} starting with test accuracy {test_accuracies[-1]}')\n",
        "\n",
        "    # Iterating over the data set and checking in with training\n",
        "    epoch_loss_agg = np.empty(0)\n",
        "    for input, target in train_dataset:\n",
        "        train_loss = train_step(model, input, target, global_loss_function, optimizer)\n",
        "        epoch_loss_agg = np.append(epoch_loss_agg, train_loss)\n",
        "\n",
        "    # Track training loss\n",
        "    train_losses = np.append(train_losses, tf.reduce_mean(epoch_loss_agg))\n",
        "    \n",
        "    # Computing train accuracy\n",
        "    _, train_accuracy = test(model, train_dataset, global_loss_function)\n",
        "    train_accuracies = np.append(train_accuracies, train_accuracy)\n",
        "\n",
        "    # Display train accuracy\n",
        "    print(f'Epoch: {str(epoch)} finishing with train accuracy {train_accuracies[-1]}')\n",
        "    print(\" \")\n",
        "\n",
        "    # Computing test loss and accuracy\n",
        "    test_loss, test_accuracy = test(model, test_dataset, global_loss_function)\n",
        "    test_losses = np.append(test_losses, test_loss)\n",
        "    test_accuracies = np.append(test_accuracies, test_accuracy)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-9a7acdf4cd6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# testing once before we begin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_loss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mtest_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mtest_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-9638d75f2a31>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, test_data, loss_function)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfor_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m#print(prediction.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0msample_test_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Exception encountered when calling layer \"dense_net\" (type DenseNet).\n\nin user code:\n\n    File \"<ipython-input-8-57471b89f076>\", line 27, in call  *\n        x = self.blocks[index](x, training=for_training)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n\n    AttributeError: Exception encountered when calling layer \"dense_block\" (type DenseBlock).\n    \n    in user code:\n    \n        File \"<ipython-input-7-c4fc526cdb44>\", line 27, in call  *\n            x = self.convolution2(x)\n    \n        AttributeError: 'DenseBlock' object has no attribute 'convolution2'\n    \n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(64, 32, 32, 32), dtype=float32)\n      • for_training=None\n\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(64, 32, 32, 3), dtype=float32)\n  • for_training=False"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqeUPCggnlU7"
      },
      "source": [
        "# Visualization and Explanation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezOd00A_npdO"
      },
      "source": [
        "# Visualize accuracy and loss for training and test data\n",
        "plt.figure()\n",
        "line1, = plt.plot(train_losses)\n",
        "line2, = plt.plot(test_losses)\n",
        "line3, = plt.plot(test_accuracies)\n",
        "line4, = plt.plot(train_accuracies)\n",
        "plt.xlabel(\"Training steps\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend((line1, line2, line3, line4), (\"training loss\", \"test loss\", \"test accuracy\", \"train accuracy\"))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}